{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b1a328",
   "metadata": {},
   "source": [
    "# Load SUPPORT dataset\n",
    "\n",
    "- fill values are found in the table below\n",
    "\n",
    "Baseline Variable | Column Name | Normal Fill-in Value\n",
    "--- | --- | ---\n",
    "Serum albumin | alb | 3.5\n",
    "PaO2/FiO2 ratio (pafi) | pafi | 333.3\n",
    "Bilirubin | bili | 1.01\n",
    "Creatinine | crea | 1.01\n",
    "BUN | bun | 6.51\n",
    "White blood count | wblc | 9 (thousands)\n",
    "Urine output | urine | 2502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "color-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import DNMC, NMC, NSurv, MLP, discrete_ci\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f2f20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>death</th>\n",
       "      <th>slos</th>\n",
       "      <th>d.time</th>\n",
       "      <th>num.co</th>\n",
       "      <th>edu</th>\n",
       "      <th>scoma</th>\n",
       "      <th>charges</th>\n",
       "      <th>totcst</th>\n",
       "      <th>totmcst</th>\n",
       "      <th>...</th>\n",
       "      <th>race_asian</th>\n",
       "      <th>race_black</th>\n",
       "      <th>race_hispanic</th>\n",
       "      <th>race_other</th>\n",
       "      <th>race_white</th>\n",
       "      <th>race_nan</th>\n",
       "      <th>ca_metastatic</th>\n",
       "      <th>ca_no</th>\n",
       "      <th>ca_yes</th>\n",
       "      <th>ca_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012772</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2029</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.253787</td>\n",
       "      <td>-0.489421</td>\n",
       "      <td>-0.487495</td>\n",
       "      <td>-0.337656</td>\n",
       "      <td>-0.274795</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.148254</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.066259</td>\n",
       "      <td>1.296607</td>\n",
       "      <td>-0.244033</td>\n",
       "      <td>-0.337656</td>\n",
       "      <td>-0.274795</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.635118</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>0.066259</td>\n",
       "      <td>-0.489421</td>\n",
       "      <td>-0.179211</td>\n",
       "      <td>-0.337656</td>\n",
       "      <td>-0.274795</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.299617</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.253787</td>\n",
       "      <td>-0.489421</td>\n",
       "      <td>-0.552730</td>\n",
       "      <td>-0.337656</td>\n",
       "      <td>-0.274795</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.105197</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2029</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066259</td>\n",
       "      <td>0.565959</td>\n",
       "      <td>-0.090466</td>\n",
       "      <td>-0.337656</td>\n",
       "      <td>-0.274795</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>0.219459</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>350</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.213925</td>\n",
       "      <td>-0.489421</td>\n",
       "      <td>-0.063517</td>\n",
       "      <td>0.116548</td>\n",
       "      <td>0.261135</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9102</th>\n",
       "      <td>-0.480760</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>347</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.253787</td>\n",
       "      <td>1.174832</td>\n",
       "      <td>-0.235378</td>\n",
       "      <td>-0.129578</td>\n",
       "      <td>-0.021128</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9103</th>\n",
       "      <td>0.495786</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066259</td>\n",
       "      <td>-0.489421</td>\n",
       "      <td>-0.125471</td>\n",
       "      <td>0.049816</td>\n",
       "      <td>0.235178</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9104</th>\n",
       "      <td>-1.002381</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386305</td>\n",
       "      <td>-0.489421</td>\n",
       "      <td>-0.008804</td>\n",
       "      <td>-0.337656</td>\n",
       "      <td>-0.274795</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>1.211265</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.213925</td>\n",
       "      <td>-0.489421</td>\n",
       "      <td>-0.429638</td>\n",
       "      <td>-0.425565</td>\n",
       "      <td>-0.305475</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9105 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age  death  slos  d.time  num.co       edu     scoma   charges  \\\n",
       "1     0.012772      0     5    2029       0 -0.253787 -0.489421 -0.487495   \n",
       "2    -0.148254      1     4       4       2  0.066259  1.296607 -0.244033   \n",
       "3    -0.635118      1    17      47       2  0.066259 -0.489421 -0.179211   \n",
       "4    -1.299617      1     3     133       2 -0.253787 -0.489421 -0.552730   \n",
       "5     1.105197      0    16    2029       1  0.066259  0.565959 -0.090466   \n",
       "...        ...    ...   ...     ...     ...       ...       ...       ...   \n",
       "9101  0.219459      0    23     350       1 -1.213925 -0.489421 -0.063517   \n",
       "9102 -0.480760      0    29     347       1 -0.253787  1.174832 -0.235378   \n",
       "9103  0.495786      0     8     346       1  0.066259 -0.489421 -0.125471   \n",
       "9104 -1.002381      1     7       7       1  0.386305 -0.489421 -0.008804   \n",
       "9105  1.211265      1    12     198       1 -1.213925 -0.489421 -0.429638   \n",
       "\n",
       "        totcst   totmcst  ...  race_asian  race_black  race_hispanic  \\\n",
       "1    -0.337656 -0.274795  ...           0           0              0   \n",
       "2    -0.337656 -0.274795  ...           0           0              0   \n",
       "3    -0.337656 -0.274795  ...           0           0              0   \n",
       "4    -0.337656 -0.274795  ...           0           0              0   \n",
       "5    -0.337656 -0.274795  ...           0           0              0   \n",
       "...        ...       ...  ...         ...         ...            ...   \n",
       "9101  0.116548  0.261135  ...           0           0              0   \n",
       "9102 -0.129578 -0.021128  ...           0           0              0   \n",
       "9103  0.049816  0.235178  ...           0           0              0   \n",
       "9104 -0.337656 -0.274795  ...           0           0              0   \n",
       "9105 -0.425565 -0.305475  ...           0           0              0   \n",
       "\n",
       "      race_other  race_white  race_nan  ca_metastatic  ca_no  ca_yes  ca_nan  \n",
       "1              1           0         0              1      0       0       0  \n",
       "2              0           1         0              0      1       0       0  \n",
       "3              0           1         0              0      1       0       0  \n",
       "4              0           1         0              1      0       0       0  \n",
       "5              0           1         0              0      1       0       0  \n",
       "...          ...         ...       ...            ...    ...     ...     ...  \n",
       "9101           0           1         0              0      1       0       0  \n",
       "9102           0           1         0              0      1       0       0  \n",
       "9103           0           1         0              0      1       0       0  \n",
       "9104           0           1         0              0      0       1       0  \n",
       "9105           0           1         0              0      1       0       0  \n",
       "\n",
       "[9105 rows x 63 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILL_VALUES = {\n",
    "    'alb': 3.5,\n",
    "    'pafi': 333.3,\n",
    "    'bili': 1.01,\n",
    "    'crea': 1.01,\n",
    "    'bun': 6.51,\n",
    "    'wblc': 9.,\n",
    "    'urine': 2502.\n",
    "}\n",
    "\n",
    "TO_DROP = ['aps', 'sps', 'surv2m', 'surv6m', 'prg2m', 'prg6m', 'dnr', 'dnrday']\n",
    "TO_DROP = TO_DROP + ['sfdm2', 'hospdead']\n",
    "\n",
    "# load, drop columns, fill using specified fill values\n",
    "df = pd.read_csv('../datasets/support2.csv').drop(TO_DROP,axis=1).fillna(value=FILL_VALUES)\n",
    "\n",
    "# get dummies for categorical vars\n",
    "df = pd.get_dummies(df, dummy_na=True)\n",
    "\n",
    "# fill remaining values to the median\n",
    "\n",
    "df = df.fillna(df.median())\n",
    "\n",
    "# standardize numeric columns\n",
    "\n",
    "numrc_cols = df.dtypes == 'float64'\n",
    "df.loc[:, numrc_cols] = (df.loc[:, numrc_cols] - df.loc[:, numrc_cols].mean()) / df.loc[:, numrc_cols].std()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b7b3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6201\n",
       "0    2904\n",
       "Name: death, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['death'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5dcd0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOMES = ['death', 'd.time']\n",
    "X = df.drop(OUTCOMES, axis=1).sample(frac=1, random_state=2021)\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a15232d",
   "metadata": {},
   "source": [
    "## We create the semi-synthetic dataset as follows\n",
    "\n",
    "- real X\n",
    "\n",
    "- make up new failure times\n",
    "- make up new failure probabilities\n",
    "- make up new censoring times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e47b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL PARAMETERS\n",
    "\n",
    "N_FEATURES = 10\n",
    "N_BINS = 10\n",
    "LEARNING_RATE = 6e-4\n",
    "N_OVERLAPPING = 5\n",
    "\n",
    "rs = np.random.RandomState(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a4a6e",
   "metadata": {},
   "source": [
    "## Separate Factors\n",
    "\n",
    "- randomly select separate factors for failure times vs probabilities (no overlap)\n",
    "- censoring times are random uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b492a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(*arrs, nbins=10, pad=1e-5):\n",
    "    min_val = np.amin(np.stack(arrs))\n",
    "    max_val = np.amax(np.stack(arrs))\n",
    "    binsize = (max_val - min_val + pad) / nbins\n",
    "    return ((arr - arr.min()) // binsize for arr in arrs)\n",
    "    \n",
    "def onehot(arr, ncategories=None):\n",
    "    if ncategories is None:\n",
    "        ncategories = len(np.unique(arr))\n",
    "    return np.eye(ncategories)[arr.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ee2819",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_order = rs.permutation(len(X.T))\n",
    "T_features = feature_order[:N_FEATURES]\n",
    "E_features = feature_order[N_FEATURES:(2 * N_FEATURES)]\n",
    "\n",
    "T_weights = rs.randn(len(T_features))\n",
    "E_weights = rs.randn(len(E_features))\n",
    "\n",
    "T = X[:, T_features] @ T_weights\n",
    "E = X[:, E_features] @ E_weights\n",
    "\n",
    "t_cont = np.exp((T - np.mean(T)) / (2 * np.std(T)))\n",
    "c_cont = rs.rand(len(t_cont)) * t_cont.max()\n",
    "\n",
    "e_prob = 1 - 1 / (1 + np.exp(4. * (E - E.mean()) / E.std()))\n",
    "e = (rs.rand(len(e_prob)) < e_prob).astype(int)\n",
    "\n",
    "ct_cont = np.stack([c_cont, t_cont]).T\n",
    "y_cont = e * np.amin(ct_cont, axis=1) + (1 - e) * c_cont\n",
    "\n",
    "s = e * np.argmin(ct_cont, axis=1)\n",
    "y_disc, t_disc, c_disc = discretize(y_cont, t_cont, c_cont, nbins=N_BINS)\n",
    "\n",
    "y = onehot(y_disc, ncategories=N_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e075b64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many are susceptible?\n",
      "0    4679\n",
      "1    4426\n",
      "dtype: int64\n",
      "How many are observed?\n",
      "0    6871\n",
      "1    2234\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('How many are susceptible?')\n",
    "print(pd.value_counts(e).sort_index())\n",
    "\n",
    "print('How many are observed?')\n",
    "print(pd.value_counts(s).sort_index())\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))\n",
    "\n",
    "ax[0].hist(e_prob)\n",
    "ax[1].hist(t_disc)\n",
    "ax[2].hist(c_disc)\n",
    "ax[3].hist(y_disc)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d193b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, s_train = X[:6000], y[:6000], s[:6000]\n",
    "x_val, y_val, s_val = X[6000:7500], y[6000:7500], s[6000:7500]\n",
    "x_test, y_test, s_test, e_test = X[7500:], y[7500:], s[7500:], e[7500:]\n",
    "t_disc_test, c_disc_test, y_disc_test = t_disc[7500:], c_disc[7500:], y_disc[7500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b19a9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(*arrs, batch_size=1):\n",
    "    l = len(arrs[0])\n",
    "    for ndx in range(0, l, batch_size):\n",
    "        yield (arr[ndx:min(ndx + batch_size, l)] for arr in arrs)\n",
    "        \n",
    "import time\n",
    "\n",
    "def train_model(\n",
    "    model, train_data, val_data, n_epochs,\n",
    "    batch_size=50, learning_rate=1e-3, early_stopping_criterion=2):\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    #@tf.function\n",
    "    def train_step(x, y, s):\n",
    "        with tf.GradientTape() as tape:\n",
    "            train_loss, train_nll = model.loss(x, y, s)\n",
    "            #print(train_loss, train_nll)\n",
    "        grads = tape.gradient(train_loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        return train_loss, train_nll\n",
    "\n",
    "    #@tf.function\n",
    "    def test_step(x, y, s):\n",
    "        val_loss, val_nll = model.loss(x, y, s)\n",
    "        return val_loss, val_nll\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    no_decrease = 0\n",
    "\n",
    "    for epoch_idx in range(n_epochs):\n",
    "\n",
    "        #print(\"\\nStart of epoch %d\" % (epoch_idx,))\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_losses = []\n",
    "        train_nlls = []\n",
    "\n",
    "        for batch_idx, (xt, yt, st) in enumerate(get_batches(*train_data, batch_size=batch_size)):\n",
    "\n",
    "            train_loss, train_nll = train_step(xt, yt, st)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            train_nlls.append(train_nll)\n",
    "\n",
    "        # Display metrics at the end of each epoch.\n",
    "        #print('Epoch training loss: %.4f, NLL = %.4f' % (np.mean(batch_losses), np.mean(batch_nll)))\n",
    "\n",
    "        val_losses = []\n",
    "        val_nlls = []\n",
    "\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        for batch_idx, (xv, yv, sv) in enumerate(get_batches(*val_data, batch_size=batch_size)):\n",
    "\n",
    "            val_loss, val_nll = test_step(xv, yv, sv)\n",
    "\n",
    "            val_losses.append(val_loss)\n",
    "            val_nlls.append(val_nll)\n",
    "            \n",
    "        new_val_loss = np.mean(val_losses)\n",
    "\n",
    "        print(\n",
    "            'Epoch %2i | Train Loss: %.4f | Train NLL: %.4f | Val Loss: %.4f | Val NLL: %.4f'\n",
    "            % (epoch_idx, np.mean(train_losses), np.mean(train_nlls), np.mean(val_losses), np.mean(val_nlls))\n",
    "        )\n",
    "        #print('Time taken: %.2fs' % (time.time() - start_time))\n",
    "                \n",
    "        if new_val_loss > best_val_loss:\n",
    "            no_decrease += 1\n",
    "        else:\n",
    "            no_decrease = 0\n",
    "            best_val_loss = new_val_loss\n",
    "            \n",
    "        if no_decrease == early_stopping_criterion:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a073e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def nll(t_true, t_pred, tol=1e-8):\n",
    "    ncat = np.shape(t_pred)[1]\n",
    "    nll_ = -1 * np.log(np.sum(onehot(t_true, ncategories=ncat) * t_pred, axis=1) + tol)\n",
    "    return np.mean(nll_)\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model, test_data, e_test, \n",
    "    batch_size=50, dataset='support', factors='unknown'):\n",
    "    \n",
    "    modelname = type(model).__name__\n",
    "    \n",
    "    test_losses = []\n",
    "    test_nlls = []\n",
    "    \n",
    "    test_e_pred = []\n",
    "    test_t_pred = []\n",
    "    test_c_pred = []\n",
    "    \n",
    "    for batch_idx, (xt, yt, st) in enumerate(get_batches(*test_data, batch_size=batch_size)):\n",
    "        \n",
    "        test_loss, test_nll = model.loss(xt, yt, st)\n",
    "        \n",
    "        if modelname == 'NSurv':\n",
    "            t_pred, c_pred = model(xt)\n",
    "            test_t_pred.append(t_pred)\n",
    "            test_c_pred.append(c_pred)\n",
    "        elif modelname == 'MLP':\n",
    "            e_pred = model(xt)\n",
    "            test_e_pred.append(e_pred)\n",
    "        else:\n",
    "            e_pred, t_pred, c_pred = model(xt)\n",
    "            test_e_pred.append(e_pred)\n",
    "            test_t_pred.append(t_pred)\n",
    "            test_c_pred.append(c_pred)\n",
    "        \n",
    "        test_losses.append(test_loss)\n",
    "        test_nlls.append(test_nll)\n",
    "    \n",
    "    if modelname == 'NSurv':\n",
    "        e_auc = None\n",
    "    else:\n",
    "        e_auc = roc_auc_score(e_test, np.concatenate(test_e_pred, axis=0))\n",
    "        \n",
    "    if modelname == 'MLP':\n",
    "        ci = None\n",
    "    else:\n",
    "        ci = discrete_ci(test_data[2], test_data[1], np.concatenate(test_t_pred, axis=0))\n",
    "    \n",
    "    results = {\n",
    "        'dataset': dataset,\n",
    "        'factors': factors,\n",
    "        'model': modelname,\n",
    "        'ld': model.ld,\n",
    "        'lr': model.lr,\n",
    "        'avg_test_loss': np.mean(test_losses),\n",
    "        'avg_test_nll': np.mean(test_nlls),\n",
    "        'e_auc': e_auc,\n",
    "        'y_ci': ci\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e43d55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN COLLECTING RESULTS HERE ###\n",
    "all_results = []\n",
    "\n",
    "pstrue = s_train.mean()\n",
    "importance_weights = [1 / (1 - pstrue), 1 / pstrue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38f2e328",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 13.9172 | Train NLL: 5.5904 | Val Loss: 10.5537 | Val NLL: 5.0827\n",
      "Epoch  1 | Train Loss: 9.2163 | Train NLL: 4.7749 | Val Loss: 8.2689 | Val NLL: 4.5612\n",
      "Epoch  2 | Train Loss: 7.6559 | Train NLL: 4.3681 | Val Loss: 7.4372 | Val NLL: 4.4988\n",
      "Epoch  3 | Train Loss: 6.9204 | Train NLL: 4.2359 | Val Loss: 6.7307 | Val NLL: 4.2785\n",
      "Epoch  4 | Train Loss: 6.3495 | Train NLL: 4.0746 | Val Loss: 6.1990 | Val NLL: 4.0909\n",
      "Epoch  5 | Train Loss: 5.9564 | Train NLL: 3.9811 | Val Loss: 6.0250 | Val NLL: 4.1787\n",
      "Epoch  6 | Train Loss: 5.6660 | Train NLL: 3.9211 | Val Loss: 5.5618 | Val NLL: 3.9200\n",
      "Epoch  7 | Train Loss: 5.4005 | Train NLL: 3.8390 | Val Loss: 5.3929 | Val NLL: 3.9156\n",
      "Epoch  8 | Train Loss: 5.1757 | Train NLL: 3.7657 | Val Loss: 5.1380 | Val NLL: 3.7994\n",
      "Epoch  9 | Train Loss: 5.0133 | Train NLL: 3.7320 | Val Loss: 5.0044 | Val NLL: 3.7828\n",
      "Epoch 10 | Train Loss: 4.8922 | Train NLL: 3.7179 | Val Loss: 4.9203 | Val NLL: 3.7961\n",
      "Epoch 11 | Train Loss: 4.7428 | Train NLL: 3.6603 | Val Loss: 4.7946 | Val NLL: 3.7588\n",
      "Epoch 12 | Train Loss: 4.6637 | Train NLL: 3.6626 | Val Loss: 4.7100 | Val NLL: 3.7485\n",
      "Epoch 13 | Train Loss: 4.5559 | Train NLL: 3.6247 | Val Loss: 4.5888 | Val NLL: 3.6917\n",
      "Epoch 14 | Train Loss: 4.4978 | Train NLL: 3.6260 | Val Loss: 4.5389 | Val NLL: 3.6977\n",
      "Epoch 15 | Train Loss: 4.4001 | Train NLL: 3.5833 | Val Loss: 4.4622 | Val NLL: 3.6738\n",
      "Epoch 16 | Train Loss: 4.3328 | Train NLL: 3.5644 | Val Loss: 4.4038 | Val NLL: 3.6599\n",
      "Epoch 17 | Train Loss: 4.2769 | Train NLL: 3.5499 | Val Loss: 4.3561 | Val NLL: 3.6512\n",
      "Epoch 18 | Train Loss: 4.2185 | Train NLL: 3.5307 | Val Loss: 4.3173 | Val NLL: 3.6503\n",
      "Epoch 19 | Train Loss: 4.1775 | Train NLL: 3.5243 | Val Loss: 4.2664 | Val NLL: 3.6316\n",
      "Epoch 20 | Train Loss: 4.1347 | Train NLL: 3.5124 | Val Loss: 4.2269 | Val NLL: 3.6211\n",
      "Epoch 21 | Train Loss: 4.1020 | Train NLL: 3.5067 | Val Loss: 4.1836 | Val NLL: 3.6025\n",
      "Epoch 22 | Train Loss: 4.0570 | Train NLL: 3.4862 | Val Loss: 4.1533 | Val NLL: 3.5969\n",
      "Epoch 23 | Train Loss: 4.0264 | Train NLL: 3.4790 | Val Loss: 4.1301 | Val NLL: 3.5955\n",
      "Epoch 24 | Train Loss: 3.9880 | Train NLL: 3.4619 | Val Loss: 4.0964 | Val NLL: 3.5822\n",
      "Epoch 25 | Train Loss: 4.0021 | Train NLL: 3.4921 | Val Loss: 4.0838 | Val NLL: 3.5834\n",
      "Epoch 26 | Train Loss: 3.9497 | Train NLL: 3.4573 | Val Loss: 4.0420 | Val NLL: 3.5601\n",
      "Epoch 27 | Train Loss: 3.9179 | Train NLL: 3.4418 | Val Loss: 4.0174 | Val NLL: 3.5503\n",
      "Epoch 28 | Train Loss: 3.8941 | Train NLL: 3.4313 | Val Loss: 3.9959 | Val NLL: 3.5420\n",
      "Epoch 29 | Train Loss: 3.8775 | Train NLL: 3.4268 | Val Loss: 3.9737 | Val NLL: 3.5306\n",
      "Epoch 30 | Train Loss: 3.8561 | Train NLL: 3.4161 | Val Loss: 3.9555 | Val NLL: 3.5236\n",
      "Epoch 31 | Train Loss: 3.8409 | Train NLL: 3.4121 | Val Loss: 3.9365 | Val NLL: 3.5153\n",
      "Epoch 32 | Train Loss: 3.8276 | Train NLL: 3.4086 | Val Loss: 3.9330 | Val NLL: 3.5202\n",
      "Epoch 33 | Train Loss: 3.8095 | Train NLL: 3.4000 | Val Loss: 3.9100 | Val NLL: 3.5076\n",
      "Epoch 34 | Train Loss: 3.7915 | Train NLL: 3.3914 | Val Loss: 3.8936 | Val NLL: 3.4994\n",
      "Epoch 35 | Train Loss: 3.7749 | Train NLL: 3.3833 | Val Loss: 3.8820 | Val NLL: 3.4962\n",
      "Epoch 36 | Train Loss: 3.7701 | Train NLL: 3.3857 | Val Loss: 3.8692 | Val NLL: 3.4899\n",
      "Epoch 37 | Train Loss: 3.7527 | Train NLL: 3.3751 | Val Loss: 3.8612 | Val NLL: 3.4884\n",
      "Epoch 38 | Train Loss: 3.7434 | Train NLL: 3.3720 | Val Loss: 3.8574 | Val NLL: 3.4906\n",
      "Epoch 39 | Train Loss: 3.7386 | Train NLL: 3.3718 | Val Loss: 3.8531 | Val NLL: 3.4907\n",
      "Epoch 40 | Train Loss: 3.7229 | Train NLL: 3.3620 | Val Loss: 3.8394 | Val NLL: 3.4834\n",
      "Epoch 41 | Train Loss: 3.7167 | Train NLL: 3.3602 | Val Loss: 3.8324 | Val NLL: 3.4793\n",
      "Epoch 42 | Train Loss: 3.7022 | Train NLL: 3.3505 | Val Loss: 3.8231 | Val NLL: 3.4754\n",
      "Epoch 43 | Train Loss: 3.6938 | Train NLL: 3.3460 | Val Loss: 3.8144 | Val NLL: 3.4711\n",
      "Epoch 44 | Train Loss: 3.6823 | Train NLL: 3.3397 | Val Loss: 3.8119 | Val NLL: 3.4725\n",
      "Epoch 45 | Train Loss: 3.6779 | Train NLL: 3.3383 | Val Loss: 3.8040 | Val NLL: 3.4682\n",
      "Epoch 46 | Train Loss: 3.6702 | Train NLL: 3.3343 | Val Loss: 3.7986 | Val NLL: 3.4664\n",
      "Epoch 47 | Train Loss: 3.6641 | Train NLL: 3.3310 | Val Loss: 3.7957 | Val NLL: 3.4657\n",
      "Epoch 48 | Train Loss: 3.6559 | Train NLL: 3.3254 | Val Loss: 3.7865 | Val NLL: 3.4592\n",
      "Epoch 49 | Train Loss: 3.6524 | Train NLL: 3.3252 | Val Loss: 3.7835 | Val NLL: 3.4590\n",
      "Epoch 50 | Train Loss: 3.6416 | Train NLL: 3.3172 | Val Loss: 3.7751 | Val NLL: 3.4534\n",
      "Epoch 51 | Train Loss: 3.6319 | Train NLL: 3.3101 | Val Loss: 3.7733 | Val NLL: 3.4543\n",
      "Epoch 52 | Train Loss: 3.6276 | Train NLL: 3.3078 | Val Loss: 3.7755 | Val NLL: 3.4580\n",
      "Epoch 53 | Train Loss: 3.6211 | Train NLL: 3.3036 | Val Loss: 3.7660 | Val NLL: 3.4514\n",
      "Epoch 54 | Train Loss: 3.6155 | Train NLL: 3.2997 | Val Loss: 3.7619 | Val NLL: 3.4481\n",
      "Epoch 55 | Train Loss: 3.6090 | Train NLL: 3.2951 | Val Loss: 3.7599 | Val NLL: 3.4484\n",
      "Epoch 56 | Train Loss: 3.6058 | Train NLL: 3.2940 | Val Loss: 3.7602 | Val NLL: 3.4500\n",
      "Epoch 57 | Train Loss: 3.6011 | Train NLL: 3.2907 | Val Loss: 3.7555 | Val NLL: 3.4483\n",
      "Epoch 58 | Train Loss: 3.5980 | Train NLL: 3.2894 | Val Loss: 3.7530 | Val NLL: 3.4469\n",
      "Epoch 59 | Train Loss: 3.5920 | Train NLL: 3.2855 | Val Loss: 3.7406 | Val NLL: 3.4363\n",
      "Epoch 60 | Train Loss: 3.5868 | Train NLL: 3.2817 | Val Loss: 3.7447 | Val NLL: 3.4411\n",
      "Epoch 61 | Train Loss: 3.5780 | Train NLL: 3.2744 | Val Loss: 3.7435 | Val NLL: 3.4420\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=1e-2, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='separate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4153f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 13.6376 | Train NLL: 5.5189 | Val Loss: 9.9465 | Val NLL: 4.8506\n",
      "Epoch  1 | Train Loss: 8.8644 | Train NLL: 4.7892 | Val Loss: 8.0929 | Val NLL: 4.7001\n",
      "Epoch  2 | Train Loss: 7.4814 | Train NLL: 4.4601 | Val Loss: 7.2880 | Val NLL: 4.5859\n",
      "Epoch  3 | Train Loss: 6.7107 | Train NLL: 4.2361 | Val Loss: 6.5416 | Val NLL: 4.2725\n",
      "Epoch  4 | Train Loss: 6.1718 | Train NLL: 4.0594 | Val Loss: 6.0588 | Val NLL: 4.0966\n",
      "Epoch  5 | Train Loss: 5.7749 | Train NLL: 3.9327 | Val Loss: 5.7371 | Val NLL: 4.0118\n",
      "Epoch  6 | Train Loss: 5.5308 | Train NLL: 3.8940 | Val Loss: 5.5086 | Val NLL: 3.9643\n",
      "Epoch  7 | Train Loss: 5.2761 | Train NLL: 3.8051 | Val Loss: 5.3341 | Val NLL: 3.9380\n",
      "Epoch  8 | Train Loss: 5.0984 | Train NLL: 3.7639 | Val Loss: 5.1436 | Val NLL: 3.8732\n",
      "Epoch  9 | Train Loss: 4.9375 | Train NLL: 3.7169 | Val Loss: 5.0215 | Val NLL: 3.8543\n",
      "Epoch 10 | Train Loss: 4.8157 | Train NLL: 3.6910 | Val Loss: 4.8897 | Val NLL: 3.8090\n",
      "Epoch 11 | Train Loss: 4.7073 | Train NLL: 3.6625 | Val Loss: 4.7359 | Val NLL: 3.7312\n",
      "Epoch 12 | Train Loss: 4.6157 | Train NLL: 3.6407 | Val Loss: 4.6681 | Val NLL: 3.7260\n",
      "Epoch 13 | Train Loss: 4.5162 | Train NLL: 3.6021 | Val Loss: 4.5935 | Val NLL: 3.7120\n",
      "Epoch 14 | Train Loss: 4.4701 | Train NLL: 3.6094 | Val Loss: 4.5783 | Val NLL: 3.7455\n",
      "Epoch 15 | Train Loss: 4.3850 | Train NLL: 3.5739 | Val Loss: 4.4850 | Val NLL: 3.6992\n",
      "Epoch 16 | Train Loss: 4.3291 | Train NLL: 3.5607 | Val Loss: 4.4375 | Val NLL: 3.6907\n",
      "Epoch 17 | Train Loss: 4.2729 | Train NLL: 3.5436 | Val Loss: 4.3775 | Val NLL: 3.6694\n",
      "Epoch 18 | Train Loss: 4.2388 | Train NLL: 3.5442 | Val Loss: 4.3617 | Val NLL: 3.6796\n",
      "Epoch 19 | Train Loss: 4.1874 | Train NLL: 3.5197 | Val Loss: 4.3026 | Val NLL: 3.6533\n",
      "Epoch 20 | Train Loss: 4.1375 | Train NLL: 3.5015 | Val Loss: 4.2469 | Val NLL: 3.6275\n",
      "Epoch 21 | Train Loss: 4.0940 | Train NLL: 3.4860 | Val Loss: 4.2111 | Val NLL: 3.6177\n",
      "Epoch 22 | Train Loss: 4.0590 | Train NLL: 3.4750 | Val Loss: 4.1718 | Val NLL: 3.6008\n",
      "Epoch 23 | Train Loss: 4.0299 | Train NLL: 3.4669 | Val Loss: 4.1285 | Val NLL: 3.5776\n",
      "Epoch 24 | Train Loss: 3.9996 | Train NLL: 3.4561 | Val Loss: 4.1151 | Val NLL: 3.5827\n",
      "Epoch 25 | Train Loss: 3.9705 | Train NLL: 3.4454 | Val Loss: 4.0833 | Val NLL: 3.5688\n",
      "Epoch 26 | Train Loss: 3.9465 | Train NLL: 3.4389 | Val Loss: 4.0606 | Val NLL: 3.5636\n",
      "Epoch 27 | Train Loss: 3.9239 | Train NLL: 3.4328 | Val Loss: 4.0400 | Val NLL: 3.5588\n",
      "Epoch 28 | Train Loss: 3.9037 | Train NLL: 3.4274 | Val Loss: 4.0166 | Val NLL: 3.5489\n",
      "Epoch 29 | Train Loss: 3.8850 | Train NLL: 3.4220 | Val Loss: 4.0024 | Val NLL: 3.5481\n",
      "Epoch 30 | Train Loss: 3.8627 | Train NLL: 3.4116 | Val Loss: 3.9839 | Val NLL: 3.5410\n",
      "Epoch 31 | Train Loss: 3.8453 | Train NLL: 3.4062 | Val Loss: 3.9600 | Val NLL: 3.5282\n",
      "Epoch 32 | Train Loss: 3.8293 | Train NLL: 3.4010 | Val Loss: 3.9517 | Val NLL: 3.5300\n",
      "Epoch 33 | Train Loss: 3.8167 | Train NLL: 3.3978 | Val Loss: 3.9384 | Val NLL: 3.5264\n",
      "Epoch 34 | Train Loss: 3.7999 | Train NLL: 3.3903 | Val Loss: 3.9130 | Val NLL: 3.5100\n",
      "Epoch 35 | Train Loss: 3.7871 | Train NLL: 3.3861 | Val Loss: 3.9099 | Val NLL: 3.5145\n",
      "Epoch 36 | Train Loss: 3.7759 | Train NLL: 3.3830 | Val Loss: 3.8961 | Val NLL: 3.5092\n",
      "Epoch 37 | Train Loss: 3.7599 | Train NLL: 3.3749 | Val Loss: 3.8722 | Val NLL: 3.4919\n",
      "Epoch 38 | Train Loss: 3.7455 | Train NLL: 3.3673 | Val Loss: 3.8637 | Val NLL: 3.4900\n",
      "Epoch 39 | Train Loss: 3.7372 | Train NLL: 3.3657 | Val Loss: 3.8623 | Val NLL: 3.4947\n",
      "Epoch 40 | Train Loss: 3.7295 | Train NLL: 3.3634 | Val Loss: 3.8613 | Val NLL: 3.4994\n",
      "Epoch 41 | Train Loss: 3.7147 | Train NLL: 3.3550 | Val Loss: 3.8367 | Val NLL: 3.4816\n",
      "Epoch 42 | Train Loss: 3.7135 | Train NLL: 3.3587 | Val Loss: 3.8213 | Val NLL: 3.4698\n",
      "Epoch 43 | Train Loss: 3.7089 | Train NLL: 3.3574 | Val Loss: 3.8394 | Val NLL: 3.4908\n",
      "Epoch 44 | Train Loss: 3.6917 | Train NLL: 3.3448 | Val Loss: 3.8150 | Val NLL: 3.4728\n",
      "Epoch 45 | Train Loss: 3.6907 | Train NLL: 3.3491 | Val Loss: 3.8018 | Val NLL: 3.4633\n",
      "Epoch 46 | Train Loss: 3.6742 | Train NLL: 3.3369 | Val Loss: 3.7962 | Val NLL: 3.4624\n",
      "Epoch 47 | Train Loss: 3.6631 | Train NLL: 3.3298 | Val Loss: 3.7946 | Val NLL: 3.4650\n",
      "Epoch 48 | Train Loss: 3.6577 | Train NLL: 3.3276 | Val Loss: 3.7907 | Val NLL: 3.4637\n",
      "Epoch 49 | Train Loss: 3.6505 | Train NLL: 3.3238 | Val Loss: 3.7865 | Val NLL: 3.4628\n",
      "Epoch 50 | Train Loss: 3.6435 | Train NLL: 3.3195 | Val Loss: 3.7798 | Val NLL: 3.4585\n",
      "Epoch 51 | Train Loss: 3.6416 | Train NLL: 3.3195 | Val Loss: 3.7834 | Val NLL: 3.4637\n",
      "Epoch 52 | Train Loss: 3.6443 | Train NLL: 3.3240 | Val Loss: 3.7754 | Val NLL: 3.4571\n",
      "Epoch 53 | Train Loss: 3.6460 | Train NLL: 3.3261 | Val Loss: 3.7820 | Val NLL: 3.4649\n",
      "Epoch 54 | Train Loss: 3.6268 | Train NLL: 3.3109 | Val Loss: 3.7688 | Val NLL: 3.4558\n",
      "Epoch 55 | Train Loss: 3.6178 | Train NLL: 3.3046 | Val Loss: 3.7563 | Val NLL: 3.4455\n",
      "Epoch 56 | Train Loss: 3.6118 | Train NLL: 3.3007 | Val Loss: 3.7524 | Val NLL: 3.4435\n",
      "Epoch 57 | Train Loss: 3.6045 | Train NLL: 3.2948 | Val Loss: 3.7468 | Val NLL: 3.4388\n",
      "Epoch 58 | Train Loss: 3.5990 | Train NLL: 3.2914 | Val Loss: 3.7430 | Val NLL: 3.4372\n",
      "Epoch 59 | Train Loss: 3.5944 | Train NLL: 3.2888 | Val Loss: 3.7463 | Val NLL: 3.4423\n",
      "Epoch 60 | Train Loss: 3.5913 | Train NLL: 3.2881 | Val Loss: 3.7424 | Val NLL: 3.4415\n",
      "Epoch 61 | Train Loss: 3.5864 | Train NLL: 3.2854 | Val Loss: 3.7458 | Val NLL: 3.4466\n",
      "Epoch 62 | Train Loss: 3.5943 | Train NLL: 3.2923 | Val Loss: 3.7491 | Val NLL: 3.4493\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=3e-3, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='separate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e6d36fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 13.5269 | Train NLL: 5.5548 | Val Loss: 10.1672 | Val NLL: 5.1312\n",
      "Epoch  1 | Train Loss: 8.8493 | Train NLL: 4.7855 | Val Loss: 8.0847 | Val NLL: 4.6894\n",
      "Epoch  2 | Train Loss: 7.4177 | Train NLL: 4.3983 | Val Loss: 7.0742 | Val NLL: 4.3698\n",
      "Epoch  3 | Train Loss: 6.6537 | Train NLL: 4.1763 | Val Loss: 6.4546 | Val NLL: 4.1875\n",
      "Epoch  4 | Train Loss: 6.1797 | Train NLL: 4.0640 | Val Loss: 6.0553 | Val NLL: 4.0916\n",
      "Epoch  5 | Train Loss: 5.7858 | Train NLL: 3.9465 | Val Loss: 5.7675 | Val NLL: 4.0476\n",
      "Epoch  6 | Train Loss: 5.4804 | Train NLL: 3.8568 | Val Loss: 5.4841 | Val NLL: 3.9539\n",
      "Epoch  7 | Train Loss: 5.2372 | Train NLL: 3.7869 | Val Loss: 5.2101 | Val NLL: 3.8405\n",
      "Epoch  8 | Train Loss: 5.0418 | Train NLL: 3.7356 | Val Loss: 5.0440 | Val NLL: 3.8015\n",
      "Epoch  9 | Train Loss: 4.8895 | Train NLL: 3.7004 | Val Loss: 4.8964 | Val NLL: 3.7626\n",
      "Epoch 10 | Train Loss: 4.7592 | Train NLL: 3.6702 | Val Loss: 4.7860 | Val NLL: 3.7444\n",
      "Epoch 11 | Train Loss: 4.6614 | Train NLL: 3.6561 | Val Loss: 4.7061 | Val NLL: 3.7400\n",
      "Epoch 12 | Train Loss: 4.5533 | Train NLL: 3.6199 | Val Loss: 4.5866 | Val NLL: 3.6889\n",
      "Epoch 13 | Train Loss: 4.4632 | Train NLL: 3.5930 | Val Loss: 4.5239 | Val NLL: 3.6826\n",
      "Epoch 14 | Train Loss: 4.3867 | Train NLL: 3.5715 | Val Loss: 4.4428 | Val NLL: 3.6548\n",
      "Epoch 15 | Train Loss: 4.3246 | Train NLL: 3.5578 | Val Loss: 4.3697 | Val NLL: 3.6256\n",
      "Epoch 16 | Train Loss: 4.2572 | Train NLL: 3.5315 | Val Loss: 4.3354 | Val NLL: 3.6304\n",
      "Epoch 17 | Train Loss: 4.2296 | Train NLL: 3.5394 | Val Loss: 4.3018 | Val NLL: 3.6260\n",
      "Epoch 18 | Train Loss: 4.1719 | Train NLL: 3.5138 | Val Loss: 4.2437 | Val NLL: 3.6046\n",
      "Epoch 19 | Train Loss: 4.1218 | Train NLL: 3.4958 | Val Loss: 4.1948 | Val NLL: 3.5846\n",
      "Epoch 20 | Train Loss: 4.0848 | Train NLL: 3.4857 | Val Loss: 4.1666 | Val NLL: 3.5812\n",
      "Epoch 21 | Train Loss: 4.0548 | Train NLL: 3.4800 | Val Loss: 4.1344 | Val NLL: 3.5721\n",
      "Epoch 22 | Train Loss: 4.0249 | Train NLL: 3.4714 | Val Loss: 4.1235 | Val NLL: 3.5826\n",
      "Epoch 23 | Train Loss: 3.9931 | Train NLL: 3.4604 | Val Loss: 4.1043 | Val NLL: 3.5826\n",
      "Epoch 24 | Train Loss: 3.9672 | Train NLL: 3.4526 | Val Loss: 4.0678 | Val NLL: 3.5633\n",
      "Epoch 25 | Train Loss: 3.9373 | Train NLL: 3.4386 | Val Loss: 4.0462 | Val NLL: 3.5573\n",
      "Epoch 26 | Train Loss: 3.9178 | Train NLL: 3.4348 | Val Loss: 4.0203 | Val NLL: 3.5455\n",
      "Epoch 27 | Train Loss: 3.8947 | Train NLL: 3.4253 | Val Loss: 4.0012 | Val NLL: 3.5397\n",
      "Epoch 28 | Train Loss: 3.8716 | Train NLL: 3.4150 | Val Loss: 3.9828 | Val NLL: 3.5336\n",
      "Epoch 29 | Train Loss: 3.8505 | Train NLL: 3.4063 | Val Loss: 3.9580 | Val NLL: 3.5211\n",
      "Epoch 30 | Train Loss: 3.8332 | Train NLL: 3.4001 | Val Loss: 3.9371 | Val NLL: 3.5106\n",
      "Epoch 31 | Train Loss: 3.8163 | Train NLL: 3.3938 | Val Loss: 3.9261 | Val NLL: 3.5092\n",
      "Epoch 32 | Train Loss: 3.8011 | Train NLL: 3.3875 | Val Loss: 3.9107 | Val NLL: 3.5028\n",
      "Epoch 33 | Train Loss: 3.7870 | Train NLL: 3.3818 | Val Loss: 3.9006 | Val NLL: 3.5005\n",
      "Epoch 34 | Train Loss: 3.7740 | Train NLL: 3.3768 | Val Loss: 3.8857 | Val NLL: 3.4935\n",
      "Epoch 35 | Train Loss: 3.7617 | Train NLL: 3.3720 | Val Loss: 3.8758 | Val NLL: 3.4905\n",
      "Epoch 36 | Train Loss: 3.7472 | Train NLL: 3.3642 | Val Loss: 3.8663 | Val NLL: 3.4880\n",
      "Epoch 37 | Train Loss: 3.7401 | Train NLL: 3.3626 | Val Loss: 3.8548 | Val NLL: 3.4822\n",
      "Epoch 38 | Train Loss: 3.7262 | Train NLL: 3.3552 | Val Loss: 3.8464 | Val NLL: 3.4799\n",
      "Epoch 39 | Train Loss: 3.7230 | Train NLL: 3.3574 | Val Loss: 3.8483 | Val NLL: 3.4854\n",
      "Epoch 40 | Train Loss: 3.7091 | Train NLL: 3.3478 | Val Loss: 3.8306 | Val NLL: 3.4736\n",
      "Epoch 41 | Train Loss: 3.6994 | Train NLL: 3.3432 | Val Loss: 3.8341 | Val NLL: 3.4809\n",
      "Epoch 42 | Train Loss: 3.6938 | Train NLL: 3.3420 | Val Loss: 3.8180 | Val NLL: 3.4699\n",
      "Epoch 43 | Train Loss: 3.6817 | Train NLL: 3.3342 | Val Loss: 3.8122 | Val NLL: 3.4684\n",
      "Epoch 44 | Train Loss: 3.6733 | Train NLL: 3.3297 | Val Loss: 3.8069 | Val NLL: 3.4665\n",
      "Epoch 45 | Train Loss: 3.6638 | Train NLL: 3.3242 | Val Loss: 3.8016 | Val NLL: 3.4651\n",
      "Epoch 46 | Train Loss: 3.6569 | Train NLL: 3.3203 | Val Loss: 3.7941 | Val NLL: 3.4600\n",
      "Epoch 47 | Train Loss: 3.6513 | Train NLL: 3.3174 | Val Loss: 3.7964 | Val NLL: 3.4649\n",
      "Epoch 48 | Train Loss: 3.6428 | Train NLL: 3.3115 | Val Loss: 3.7866 | Val NLL: 3.4580\n",
      "Epoch 49 | Train Loss: 3.6357 | Train NLL: 3.3071 | Val Loss: 3.7854 | Val NLL: 3.4590\n",
      "Epoch 50 | Train Loss: 3.6267 | Train NLL: 3.3001 | Val Loss: 3.7816 | Val NLL: 3.4572\n",
      "Epoch 51 | Train Loss: 3.6214 | Train NLL: 3.2977 | Val Loss: 3.7731 | Val NLL: 3.4517\n",
      "Epoch 52 | Train Loss: 3.6156 | Train NLL: 3.2938 | Val Loss: 3.7720 | Val NLL: 3.4527\n",
      "Epoch 53 | Train Loss: 3.6107 | Train NLL: 3.2902 | Val Loss: 3.7729 | Val NLL: 3.4549\n",
      "Epoch 54 | Train Loss: 3.6015 | Train NLL: 3.2837 | Val Loss: 3.7689 | Val NLL: 3.4531\n",
      "Epoch 55 | Train Loss: 3.5971 | Train NLL: 3.2809 | Val Loss: 3.7589 | Val NLL: 3.4442\n",
      "Epoch 56 | Train Loss: 3.5942 | Train NLL: 3.2785 | Val Loss: 3.7595 | Val NLL: 3.4457\n",
      "Epoch 57 | Train Loss: 3.5903 | Train NLL: 3.2771 | Val Loss: 3.7589 | Val NLL: 3.4475\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=3e-2, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='separate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34a888d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 12.2385 | Train NLL: 5.3304 | Val Loss: 9.2848 | Val NLL: 4.7426\n",
      "Epoch  1 | Train Loss: 8.2173 | Train NLL: 4.6215 | Val Loss: 7.3261 | Val NLL: 4.4022\n",
      "Epoch  2 | Train Loss: 6.9008 | Train NLL: 4.3458 | Val Loss: 6.5140 | Val NLL: 4.2632\n",
      "Epoch  3 | Train Loss: 6.2406 | Train NLL: 4.1997 | Val Loss: 6.0121 | Val NLL: 4.1567\n",
      "Epoch  4 | Train Loss: 5.7983 | Train NLL: 4.0846 | Val Loss: 5.6558 | Val NLL: 4.0739\n",
      "Epoch  5 | Train Loss: 5.4813 | Train NLL: 4.0038 | Val Loss: 5.3768 | Val NLL: 4.0001\n",
      "Epoch  6 | Train Loss: 5.2853 | Train NLL: 3.9880 | Val Loss: 5.1684 | Val NLL: 3.9492\n",
      "Epoch  7 | Train Loss: 5.0823 | Train NLL: 3.9271 | Val Loss: 5.0538 | Val NLL: 3.9607\n",
      "Epoch  8 | Train Loss: 4.9007 | Train NLL: 3.8600 | Val Loss: 4.8421 | Val NLL: 3.8554\n",
      "Epoch  9 | Train Loss: 4.7976 | Train NLL: 3.8538 | Val Loss: 4.8458 | Val NLL: 3.9445\n",
      "Epoch 10 | Train Loss: 4.7178 | Train NLL: 3.8505 | Val Loss: 4.6583 | Val NLL: 3.8298\n",
      "Epoch 11 | Train Loss: 4.6079 | Train NLL: 3.8089 | Val Loss: 4.5825 | Val NLL: 3.8163\n",
      "Epoch 12 | Train Loss: 4.5369 | Train NLL: 3.7959 | Val Loss: 4.5116 | Val NLL: 3.7989\n",
      "Epoch 13 | Train Loss: 4.4738 | Train NLL: 3.7823 | Val Loss: 4.4607 | Val NLL: 3.7944\n",
      "Epoch 14 | Train Loss: 4.4210 | Train NLL: 3.7728 | Val Loss: 4.4105 | Val NLL: 3.7843\n",
      "Epoch 15 | Train Loss: 4.3688 | Train NLL: 3.7584 | Val Loss: 4.3660 | Val NLL: 3.7752\n",
      "Epoch 16 | Train Loss: 4.3219 | Train NLL: 3.7447 | Val Loss: 4.3273 | Val NLL: 3.7675\n",
      "Epoch 17 | Train Loss: 4.2838 | Train NLL: 3.7359 | Val Loss: 4.2931 | Val NLL: 3.7606\n",
      "Epoch 18 | Train Loss: 4.2469 | Train NLL: 3.7246 | Val Loss: 4.2642 | Val NLL: 3.7557\n",
      "Epoch 19 | Train Loss: 4.2131 | Train NLL: 3.7137 | Val Loss: 4.2411 | Val NLL: 3.7542\n",
      "Epoch 20 | Train Loss: 4.1857 | Train NLL: 3.7070 | Val Loss: 4.2144 | Val NLL: 3.7467\n",
      "Epoch 21 | Train Loss: 4.1595 | Train NLL: 3.6988 | Val Loss: 4.1952 | Val NLL: 3.7450\n",
      "Epoch 22 | Train Loss: 4.1364 | Train NLL: 3.6925 | Val Loss: 4.1711 | Val NLL: 3.7367\n",
      "Epoch 23 | Train Loss: 4.1131 | Train NLL: 3.6841 | Val Loss: 4.1538 | Val NLL: 3.7333\n",
      "Epoch 24 | Train Loss: 4.0949 | Train NLL: 3.6792 | Val Loss: 4.1355 | Val NLL: 3.7273\n",
      "Epoch 25 | Train Loss: 4.0774 | Train NLL: 3.6730 | Val Loss: 4.1208 | Val NLL: 3.7241\n",
      "Epoch 26 | Train Loss: 4.0592 | Train NLL: 3.6661 | Val Loss: 4.1095 | Val NLL: 3.7229\n",
      "Epoch 27 | Train Loss: 4.0459 | Train NLL: 3.6625 | Val Loss: 4.0946 | Val NLL: 3.7175\n",
      "Epoch 28 | Train Loss: 4.0287 | Train NLL: 3.6543 | Val Loss: 4.0841 | Val NLL: 3.7152\n",
      "Epoch 29 | Train Loss: 4.0175 | Train NLL: 3.6509 | Val Loss: 4.0713 | Val NLL: 3.7102\n",
      "Epoch 30 | Train Loss: 4.0043 | Train NLL: 3.6454 | Val Loss: 4.0627 | Val NLL: 3.7089\n",
      "Epoch 31 | Train Loss: 3.9932 | Train NLL: 3.6410 | Val Loss: 4.0555 | Val NLL: 3.7080\n",
      "Epoch 32 | Train Loss: 3.9832 | Train NLL: 3.6369 | Val Loss: 4.0476 | Val NLL: 3.7057\n",
      "Epoch 33 | Train Loss: 3.9730 | Train NLL: 3.6323 | Val Loss: 4.0402 | Val NLL: 3.7031\n",
      "Epoch 34 | Train Loss: 3.9629 | Train NLL: 3.6274 | Val Loss: 4.0335 | Val NLL: 3.7019\n",
      "Epoch 35 | Train Loss: 3.9536 | Train NLL: 3.6232 | Val Loss: 4.0260 | Val NLL: 3.6993\n",
      "Epoch 36 | Train Loss: 3.9433 | Train NLL: 3.6173 | Val Loss: 4.0193 | Val NLL: 3.6969\n",
      "Epoch 37 | Train Loss: 3.9345 | Train NLL: 3.6128 | Val Loss: 4.0165 | Val NLL: 3.6979\n",
      "Epoch 38 | Train Loss: 3.9262 | Train NLL: 3.6083 | Val Loss: 4.0082 | Val NLL: 3.6937\n",
      "Epoch 39 | Train Loss: 3.9190 | Train NLL: 3.6047 | Val Loss: 4.0070 | Val NLL: 3.6951\n",
      "Epoch 40 | Train Loss: 3.9130 | Train NLL: 3.6016 | Val Loss: 3.9985 | Val NLL: 3.6900\n",
      "Epoch 41 | Train Loss: 3.9032 | Train NLL: 3.5948 | Val Loss: 3.9990 | Val NLL: 3.6937\n",
      "Epoch 42 | Train Loss: 3.8978 | Train NLL: 3.5928 | Val Loss: 3.9934 | Val NLL: 3.6906\n",
      "Epoch 43 | Train Loss: 3.8907 | Train NLL: 3.5881 | Val Loss: 3.9931 | Val NLL: 3.6929\n",
      "Epoch 44 | Train Loss: 3.8839 | Train NLL: 3.5840 | Val Loss: 3.9837 | Val NLL: 3.6864\n",
      "Epoch 45 | Train Loss: 3.8765 | Train NLL: 3.5793 | Val Loss: 3.9806 | Val NLL: 3.6853\n",
      "Epoch 46 | Train Loss: 3.8699 | Train NLL: 3.5748 | Val Loss: 3.9769 | Val NLL: 3.6843\n",
      "Epoch 47 | Train Loss: 3.8638 | Train NLL: 3.5714 | Val Loss: 3.9795 | Val NLL: 3.6885\n",
      "Epoch 48 | Train Loss: 3.8586 | Train NLL: 3.5677 | Val Loss: 3.9719 | Val NLL: 3.6830\n",
      "Epoch 49 | Train Loss: 3.8517 | Train NLL: 3.5631 | Val Loss: 3.9677 | Val NLL: 3.6810\n",
      "Epoch 50 | Train Loss: 3.8460 | Train NLL: 3.5596 | Val Loss: 3.9630 | Val NLL: 3.6789\n",
      "Epoch 51 | Train Loss: 3.8402 | Train NLL: 3.5559 | Val Loss: 3.9643 | Val NLL: 3.6817\n",
      "Epoch 52 | Train Loss: 3.8359 | Train NLL: 3.5532 | Val Loss: 3.9593 | Val NLL: 3.6784\n",
      "Epoch 53 | Train Loss: 3.8291 | Train NLL: 3.5483 | Val Loss: 3.9577 | Val NLL: 3.6787\n",
      "Epoch 54 | Train Loss: 3.8257 | Train NLL: 3.5465 | Val Loss: 3.9544 | Val NLL: 3.6768\n",
      "Epoch 55 | Train Loss: 3.8204 | Train NLL: 3.5427 | Val Loss: 3.9541 | Val NLL: 3.6784\n",
      "Epoch 56 | Train Loss: 3.8171 | Train NLL: 3.5410 | Val Loss: 3.9518 | Val NLL: 3.6775\n",
      "Epoch 57 | Train Loss: 3.8127 | Train NLL: 3.5375 | Val Loss: 3.9516 | Val NLL: 3.6773\n",
      "Epoch 58 | Train Loss: 3.8090 | Train NLL: 3.5348 | Val Loss: 3.9448 | Val NLL: 3.6723\n",
      "Epoch 59 | Train Loss: 3.8028 | Train NLL: 3.5303 | Val Loss: 3.9481 | Val NLL: 3.6773\n",
      "Epoch 60 | Train Loss: 3.7996 | Train NLL: 3.5283 | Val Loss: 3.9420 | Val NLL: 3.6724\n",
      "Epoch 61 | Train Loss: 3.7945 | Train NLL: 3.5246 | Val Loss: 3.9430 | Val NLL: 3.6744\n",
      "Epoch 62 | Train Loss: 3.7890 | Train NLL: 3.5204 | Val Loss: 3.9368 | Val NLL: 3.6700\n",
      "Epoch 63 | Train Loss: 3.7837 | Train NLL: 3.5165 | Val Loss: 3.9348 | Val NLL: 3.6689\n",
      "Epoch 64 | Train Loss: 3.7809 | Train NLL: 3.5145 | Val Loss: 3.9330 | Val NLL: 3.6683\n",
      "Epoch 65 | Train Loss: 3.7759 | Train NLL: 3.5105 | Val Loss: 3.9323 | Val NLL: 3.6681\n",
      "Epoch 66 | Train Loss: 3.7721 | Train NLL: 3.5080 | Val Loss: 3.9314 | Val NLL: 3.6684\n",
      "Epoch 67 | Train Loss: 3.7692 | Train NLL: 3.5059 | Val Loss: 3.9272 | Val NLL: 3.6650\n",
      "Epoch 68 | Train Loss: 3.7636 | Train NLL: 3.5011 | Val Loss: 3.9296 | Val NLL: 3.6684\n",
      "Epoch 69 | Train Loss: 3.7601 | Train NLL: 3.4983 | Val Loss: 3.9287 | Val NLL: 3.6680\n"
     ]
    }
   ],
   "source": [
    "model = NMC(n_bins=N_BINS, importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='separate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75f46290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 10.3683 | Train NLL: 5.4280 | Val Loss: 8.4810 | Val NLL: 4.9561\n",
      "Epoch  1 | Train Loss: 7.6371 | Train NLL: 4.7348 | Val Loss: 7.1020 | Val NLL: 4.6617\n",
      "Epoch  2 | Train Loss: 6.6410 | Train NLL: 4.4645 | Val Loss: 6.4124 | Val NLL: 4.4604\n",
      "Epoch  3 | Train Loss: 6.1072 | Train NLL: 4.3105 | Val Loss: 6.0098 | Val NLL: 4.3556\n",
      "Epoch  4 | Train Loss: 5.7368 | Train NLL: 4.1894 | Val Loss: 5.7108 | Val NLL: 4.2658\n",
      "Epoch  5 | Train Loss: 5.4657 | Train NLL: 4.0999 | Val Loss: 5.3946 | Val NLL: 4.1079\n",
      "Epoch  6 | Train Loss: 5.2492 | Train NLL: 4.0246 | Val Loss: 5.1707 | Val NLL: 4.0100\n",
      "Epoch  7 | Train Loss: 5.0769 | Train NLL: 3.9660 | Val Loss: 5.0257 | Val NLL: 3.9674\n",
      "Epoch  8 | Train Loss: 4.9398 | Train NLL: 3.9224 | Val Loss: 4.9019 | Val NLL: 3.9289\n",
      "Epoch  9 | Train Loss: 4.8237 | Train NLL: 3.8856 | Val Loss: 4.8074 | Val NLL: 3.9076\n",
      "Epoch 10 | Train Loss: 4.7278 | Train NLL: 3.8573 | Val Loss: 4.7208 | Val NLL: 3.8837\n",
      "Epoch 11 | Train Loss: 4.6397 | Train NLL: 3.8280 | Val Loss: 4.6409 | Val NLL: 3.8582\n",
      "Epoch 12 | Train Loss: 4.5657 | Train NLL: 3.8052 | Val Loss: 4.5744 | Val NLL: 3.8396\n",
      "Epoch 13 | Train Loss: 4.5002 | Train NLL: 3.7849 | Val Loss: 4.5189 | Val NLL: 3.8269\n",
      "Epoch 14 | Train Loss: 4.4424 | Train NLL: 3.7673 | Val Loss: 4.4640 | Val NLL: 3.8093\n",
      "Epoch 15 | Train Loss: 4.3894 | Train NLL: 3.7496 | Val Loss: 4.4205 | Val NLL: 3.7993\n",
      "Epoch 16 | Train Loss: 4.3429 | Train NLL: 3.7350 | Val Loss: 4.3785 | Val NLL: 3.7876\n",
      "Epoch 17 | Train Loss: 4.3036 | Train NLL: 3.7242 | Val Loss: 4.3492 | Val NLL: 3.7847\n",
      "Epoch 18 | Train Loss: 4.2639 | Train NLL: 3.7099 | Val Loss: 4.3087 | Val NLL: 3.7684\n",
      "Epoch 19 | Train Loss: 4.2330 | Train NLL: 3.7018 | Val Loss: 4.2796 | Val NLL: 3.7607\n",
      "Epoch 20 | Train Loss: 4.2034 | Train NLL: 3.6928 | Val Loss: 4.2565 | Val NLL: 3.7577\n",
      "Epoch 21 | Train Loss: 4.1715 | Train NLL: 3.6799 | Val Loss: 4.2296 | Val NLL: 3.7482\n",
      "Epoch 22 | Train Loss: 4.1465 | Train NLL: 3.6715 | Val Loss: 4.2079 | Val NLL: 3.7424\n",
      "Epoch 23 | Train Loss: 4.1219 | Train NLL: 3.6625 | Val Loss: 4.1883 | Val NLL: 3.7381\n",
      "Epoch 24 | Train Loss: 4.1003 | Train NLL: 3.6551 | Val Loss: 4.1707 | Val NLL: 3.7338\n",
      "Epoch 25 | Train Loss: 4.0793 | Train NLL: 3.6469 | Val Loss: 4.1539 | Val NLL: 3.7295\n",
      "Epoch 26 | Train Loss: 4.0626 | Train NLL: 3.6421 | Val Loss: 4.1402 | Val NLL: 3.7261\n",
      "Epoch 27 | Train Loss: 4.0432 | Train NLL: 3.6332 | Val Loss: 4.1233 | Val NLL: 3.7199\n",
      "Epoch 28 | Train Loss: 4.0256 | Train NLL: 3.6256 | Val Loss: 4.1104 | Val NLL: 3.7166\n",
      "Epoch 29 | Train Loss: 4.0113 | Train NLL: 3.6203 | Val Loss: 4.1023 | Val NLL: 3.7169\n",
      "Epoch 30 | Train Loss: 3.9972 | Train NLL: 3.6142 | Val Loss: 4.0915 | Val NLL: 3.7138\n",
      "Epoch 31 | Train Loss: 3.9828 | Train NLL: 3.6073 | Val Loss: 4.0875 | Val NLL: 3.7168\n",
      "Epoch 32 | Train Loss: 3.9730 | Train NLL: 3.6040 | Val Loss: 4.0711 | Val NLL: 3.7069\n",
      "Epoch 33 | Train Loss: 3.9580 | Train NLL: 3.5958 | Val Loss: 4.0606 | Val NLL: 3.7027\n",
      "Epoch 34 | Train Loss: 3.9459 | Train NLL: 3.5891 | Val Loss: 4.0558 | Val NLL: 3.7028\n",
      "Epoch 35 | Train Loss: 3.9369 | Train NLL: 3.5852 | Val Loss: 4.0469 | Val NLL: 3.6990\n",
      "Epoch 36 | Train Loss: 3.9269 | Train NLL: 3.5803 | Val Loss: 4.0375 | Val NLL: 3.6944\n",
      "Epoch 37 | Train Loss: 3.9158 | Train NLL: 3.5733 | Val Loss: 4.0350 | Val NLL: 3.6962\n",
      "Epoch 38 | Train Loss: 3.9065 | Train NLL: 3.5687 | Val Loss: 4.0293 | Val NLL: 3.6947\n",
      "Epoch 39 | Train Loss: 3.8962 | Train NLL: 3.5624 | Val Loss: 4.0220 | Val NLL: 3.6910\n",
      "Epoch 40 | Train Loss: 3.8896 | Train NLL: 3.5582 | Val Loss: 4.0166 | Val NLL: 3.6881\n",
      "Epoch 41 | Train Loss: 3.8811 | Train NLL: 3.5532 | Val Loss: 4.0126 | Val NLL: 3.6875\n",
      "Epoch 42 | Train Loss: 3.8732 | Train NLL: 3.5479 | Val Loss: 4.0120 | Val NLL: 3.6896\n",
      "Epoch 43 | Train Loss: 3.8644 | Train NLL: 3.5424 | Val Loss: 4.0044 | Val NLL: 3.6850\n",
      "Epoch 44 | Train Loss: 3.8568 | Train NLL: 3.5376 | Val Loss: 4.0038 | Val NLL: 3.6861\n",
      "Epoch 45 | Train Loss: 3.8490 | Train NLL: 3.5318 | Val Loss: 3.9998 | Val NLL: 3.6848\n",
      "Epoch 46 | Train Loss: 3.8443 | Train NLL: 3.5293 | Val Loss: 4.0010 | Val NLL: 3.6881\n",
      "Epoch 47 | Train Loss: 3.8371 | Train NLL: 3.5240 | Val Loss: 3.9934 | Val NLL: 3.6821\n",
      "Epoch 48 | Train Loss: 3.8299 | Train NLL: 3.5189 | Val Loss: 3.9943 | Val NLL: 3.6851\n",
      "Epoch 49 | Train Loss: 3.8234 | Train NLL: 3.5142 | Val Loss: 3.9910 | Val NLL: 3.6837\n",
      "Epoch 50 | Train Loss: 3.8195 | Train NLL: 3.5117 | Val Loss: 3.9905 | Val NLL: 3.6845\n",
      "Epoch 51 | Train Loss: 3.8113 | Train NLL: 3.5056 | Val Loss: 3.9837 | Val NLL: 3.6798\n",
      "Epoch 52 | Train Loss: 3.8068 | Train NLL: 3.5024 | Val Loss: 3.9827 | Val NLL: 3.6800\n",
      "Epoch 53 | Train Loss: 3.7983 | Train NLL: 3.4953 | Val Loss: 3.9797 | Val NLL: 3.6787\n",
      "Epoch 54 | Train Loss: 3.7937 | Train NLL: 3.4923 | Val Loss: 3.9788 | Val NLL: 3.6788\n",
      "Epoch 55 | Train Loss: 3.7886 | Train NLL: 3.4884 | Val Loss: 3.9774 | Val NLL: 3.6788\n",
      "Epoch 56 | Train Loss: 3.7838 | Train NLL: 3.4846 | Val Loss: 3.9758 | Val NLL: 3.6781\n",
      "Epoch 57 | Train Loss: 3.7786 | Train NLL: 3.4804 | Val Loss: 3.9788 | Val NLL: 3.6822\n",
      "Epoch 58 | Train Loss: 3.7743 | Train NLL: 3.4770 | Val Loss: 3.9785 | Val NLL: 3.6823\n"
     ]
    }
   ],
   "source": [
    "model = NSurv(n_bins=N_BINS, importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='separate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa41bc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 3.5809 | Train NLL: 1.2390 | Val Loss: 2.6526 | Val NLL: 1.0909\n",
      "Epoch  1 | Train Loss: 2.3427 | Train NLL: 1.1014 | Val Loss: 2.0575 | Val NLL: 1.0434\n",
      "Epoch  2 | Train Loss: 1.9639 | Train NLL: 1.0739 | Val Loss: 1.8125 | Val NLL: 1.0253\n",
      "Epoch  3 | Train Loss: 1.7687 | Train NLL: 1.0528 | Val Loss: 1.6645 | Val NLL: 1.0119\n",
      "Epoch  4 | Train Loss: 1.6443 | Train NLL: 1.0410 | Val Loss: 1.5665 | Val NLL: 1.0082\n",
      "Epoch  5 | Train Loss: 1.5485 | Train NLL: 1.0267 | Val Loss: 1.4888 | Val NLL: 1.0010\n",
      "Epoch  6 | Train Loss: 1.4698 | Train NLL: 1.0110 | Val Loss: 1.4278 | Val NLL: 0.9958\n",
      "Epoch  7 | Train Loss: 1.4088 | Train NLL: 1.0002 | Val Loss: 1.3757 | Val NLL: 0.9893\n",
      "Epoch  8 | Train Loss: 1.3615 | Train NLL: 0.9944 | Val Loss: 1.3345 | Val NLL: 0.9856\n",
      "Epoch  9 | Train Loss: 1.3317 | Train NLL: 0.9991 | Val Loss: 1.3196 | Val NLL: 1.0021\n",
      "Epoch 10 | Train Loss: 1.2856 | Train NLL: 0.9822 | Val Loss: 1.2736 | Val NLL: 0.9842\n",
      "Epoch 11 | Train Loss: 1.2546 | Train NLL: 0.9780 | Val Loss: 1.2475 | Val NLL: 0.9826\n",
      "Epoch 12 | Train Loss: 1.2267 | Train NLL: 0.9727 | Val Loss: 1.2207 | Val NLL: 0.9768\n",
      "Epoch 13 | Train Loss: 1.2089 | Train NLL: 0.9748 | Val Loss: 1.2056 | Val NLL: 0.9801\n",
      "Epoch 14 | Train Loss: 1.1854 | Train NLL: 0.9686 | Val Loss: 1.1803 | Val NLL: 0.9719\n",
      "Epoch 15 | Train Loss: 1.1733 | Train NLL: 0.9727 | Val Loss: 1.1741 | Val NLL: 0.9803\n",
      "Epoch 16 | Train Loss: 1.1514 | Train NLL: 0.9648 | Val Loss: 1.1489 | Val NLL: 0.9690\n",
      "Epoch 17 | Train Loss: 1.1376 | Train NLL: 0.9641 | Val Loss: 1.1393 | Val NLL: 0.9715\n",
      "Epoch 18 | Train Loss: 1.1229 | Train NLL: 0.9611 | Val Loss: 1.1233 | Val NLL: 0.9667\n",
      "Epoch 19 | Train Loss: 1.1113 | Train NLL: 0.9604 | Val Loss: 1.1155 | Val NLL: 0.9694\n",
      "Epoch 20 | Train Loss: 1.0993 | Train NLL: 0.9581 | Val Loss: 1.1042 | Val NLL: 0.9670\n",
      "Epoch 21 | Train Loss: 1.0898 | Train NLL: 0.9575 | Val Loss: 1.0933 | Val NLL: 0.9646\n",
      "Epoch 22 | Train Loss: 1.0801 | Train NLL: 0.9559 | Val Loss: 1.0842 | Val NLL: 0.9635\n",
      "Epoch 23 | Train Loss: 1.0708 | Train NLL: 0.9541 | Val Loss: 1.0751 | Val NLL: 0.9615\n",
      "Epoch 24 | Train Loss: 1.0633 | Train NLL: 0.9534 | Val Loss: 1.0701 | Val NLL: 0.9628\n",
      "Epoch 25 | Train Loss: 1.0573 | Train NLL: 0.9534 | Val Loss: 1.0647 | Val NLL: 0.9631\n",
      "Epoch 26 | Train Loss: 1.0485 | Train NLL: 0.9500 | Val Loss: 1.0556 | Val NLL: 0.9591\n",
      "Epoch 27 | Train Loss: 1.0420 | Train NLL: 0.9483 | Val Loss: 1.0515 | Val NLL: 0.9596\n",
      "Epoch 28 | Train Loss: 1.0375 | Train NLL: 0.9482 | Val Loss: 1.0490 | Val NLL: 0.9614\n",
      "Epoch 29 | Train Loss: 1.0313 | Train NLL: 0.9461 | Val Loss: 1.0448 | Val NLL: 0.9613\n",
      "Epoch 30 | Train Loss: 1.0274 | Train NLL: 0.9459 | Val Loss: 1.0400 | Val NLL: 0.9599\n",
      "Epoch 31 | Train Loss: 1.0217 | Train NLL: 0.9437 | Val Loss: 1.0367 | Val NLL: 0.9602\n",
      "Epoch 32 | Train Loss: 1.0172 | Train NLL: 0.9423 | Val Loss: 1.0315 | Val NLL: 0.9574\n",
      "Epoch 33 | Train Loss: 1.0151 | Train NLL: 0.9422 | Val Loss: 1.0312 | Val NLL: 0.9592\n",
      "Epoch 34 | Train Loss: 1.0100 | Train NLL: 0.9395 | Val Loss: 1.0302 | Val NLL: 0.9609\n",
      "Epoch 35 | Train Loss: 1.0061 | Train NLL: 0.9379 | Val Loss: 1.0270 | Val NLL: 0.9597\n",
      "Epoch 36 | Train Loss: 1.0027 | Train NLL: 0.9363 | Val Loss: 1.0248 | Val NLL: 0.9592\n",
      "Epoch 37 | Train Loss: 0.9998 | Train NLL: 0.9347 | Val Loss: 1.0229 | Val NLL: 0.9584\n",
      "Epoch 38 | Train Loss: 0.9968 | Train NLL: 0.9327 | Val Loss: 1.0218 | Val NLL: 0.9583\n",
      "Epoch 39 | Train Loss: 0.9949 | Train NLL: 0.9318 | Val Loss: 1.0213 | Val NLL: 0.9590\n",
      "Epoch 40 | Train Loss: 0.9918 | Train NLL: 0.9300 | Val Loss: 1.0196 | Val NLL: 0.9584\n",
      "Epoch 41 | Train Loss: 0.9894 | Train NLL: 0.9285 | Val Loss: 1.0185 | Val NLL: 0.9579\n",
      "Epoch 42 | Train Loss: 0.9875 | Train NLL: 0.9273 | Val Loss: 1.0184 | Val NLL: 0.9586\n",
      "Epoch 43 | Train Loss: 0.9848 | Train NLL: 0.9251 | Val Loss: 1.0182 | Val NLL: 0.9590\n",
      "Epoch 44 | Train Loss: 0.9830 | Train NLL: 0.9237 | Val Loss: 1.0180 | Val NLL: 0.9590\n",
      "Epoch 45 | Train Loss: 0.9815 | Train NLL: 0.9226 | Val Loss: 1.0177 | Val NLL: 0.9589\n",
      "Epoch 46 | Train Loss: 0.9799 | Train NLL: 0.9213 | Val Loss: 1.0171 | Val NLL: 0.9588\n",
      "Epoch 47 | Train Loss: 0.9779 | Train NLL: 0.9197 | Val Loss: 1.0171 | Val NLL: 0.9592\n",
      "Epoch 48 | Train Loss: 0.9767 | Train NLL: 0.9187 | Val Loss: 1.0184 | Val NLL: 0.9601\n"
     ]
    }
   ],
   "source": [
    "model = MLP(importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='separate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2be2d2",
   "metadata": {},
   "source": [
    "## Overlapping Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e7941b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_order = rs.permutation(len(X.T))\n",
    "T_features = feature_order[N_OVERLAPPING:(N_FEATURES + N_OVERLAPPING)]\n",
    "E_features = feature_order[N_FEATURES:(2 * N_FEATURES)]\n",
    "\n",
    "T_weights = rs.randn(len(T_features))\n",
    "E_weights = rs.randn(len(E_features))\n",
    "\n",
    "T = X[:, T_features] @ T_weights\n",
    "E = X[:, E_features] @ E_weights\n",
    "\n",
    "t_cont = np.exp((T - np.mean(T)) / (2 * np.std(T)))\n",
    "c_cont = rs.rand(len(t_cont)) * t_cont.max()\n",
    "\n",
    "e_prob = 1 - 1 / (1 + np.exp(4. * (E - E.mean()) / E.std()))\n",
    "e = (rs.rand(len(e_prob)) < e_prob).astype(int)\n",
    "\n",
    "ct_cont = np.stack([c_cont, t_cont]).T\n",
    "y_cont = e * np.amin(ct_cont, axis=1) + (1 - e) * c_cont\n",
    "\n",
    "s = e * np.argmin(ct_cont, axis=1)\n",
    "y_disc, t_disc, c_disc = discretize(y_cont, t_cont, c_cont, nbins=N_BINS)\n",
    "\n",
    "y = onehot(y_disc, ncategories=N_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5802e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pstrue = s_train.mean()\n",
    "importance_weights = [1 / (1 - pstrue), 1 / pstrue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40952645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many are susceptible?\n",
      "0    5107\n",
      "1    3998\n",
      "dtype: int64\n",
      "How many are observed?\n",
      "0    5948\n",
      "1    3157\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('How many are susceptible?')\n",
    "print(pd.value_counts(e).sort_index())\n",
    "\n",
    "print('How many are observed?')\n",
    "print(pd.value_counts(s).sort_index())\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))\n",
    "\n",
    "ax[0].hist(e_prob)\n",
    "ax[1].hist(t_disc)\n",
    "ax[2].hist(c_disc)\n",
    "ax[3].hist(y_disc)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4adfdc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, s_train = X[:6000], y[:6000], s[:6000]\n",
    "x_val, y_val, s_val = X[6000:7500], y[6000:7500], s[6000:7500]\n",
    "x_test, y_test, s_test, e_test = X[7500:], y[7500:], s[7500:], e[7500:]\n",
    "t_disc_test, c_disc_test, y_disc_test = t_disc[7500:], c_disc[7500:], y_disc[7500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5954475f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6000, 61), (6000, 10), (6000,), (1500, 61), (1500, 10), (1500,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, s_train.shape, x_val.shape, y_val.shape, s_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d17f5ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 13.7682 | Train NLL: 5.3611 | Val Loss: 10.3853 | Val NLL: 4.8976\n",
      "Epoch  1 | Train Loss: 8.9239 | Train NLL: 4.5424 | Val Loss: 8.0021 | Val NLL: 4.4032\n",
      "Epoch  2 | Train Loss: 7.3707 | Train NLL: 4.2129 | Val Loss: 7.0328 | Val NLL: 4.2411\n",
      "Epoch  3 | Train Loss: 6.5846 | Train NLL: 4.0388 | Val Loss: 6.3813 | Val NLL: 4.0536\n",
      "Epoch  4 | Train Loss: 6.0580 | Train NLL: 3.9018 | Val Loss: 5.9021 | Val NLL: 3.8984\n",
      "Epoch  5 | Train Loss: 5.6396 | Train NLL: 3.7575 | Val Loss: 5.5774 | Val NLL: 3.8085\n",
      "Epoch  6 | Train Loss: 5.3754 | Train NLL: 3.6995 | Val Loss: 5.3342 | Val NLL: 3.7459\n",
      "Epoch  7 | Train Loss: 5.1288 | Train NLL: 3.6121 | Val Loss: 5.1042 | Val NLL: 3.6597\n",
      "Epoch  8 | Train Loss: 4.9263 | Train NLL: 3.5400 | Val Loss: 4.9598 | Val NLL: 3.6328\n",
      "Epoch  9 | Train Loss: 4.7727 | Train NLL: 3.4962 | Val Loss: 4.8555 | Val NLL: 3.6286\n",
      "Epoch 10 | Train Loss: 4.6422 | Train NLL: 3.4568 | Val Loss: 4.7015 | Val NLL: 3.5573\n",
      "Epoch 11 | Train Loss: 4.5146 | Train NLL: 3.4069 | Val Loss: 4.6142 | Val NLL: 3.5421\n",
      "Epoch 12 | Train Loss: 4.4088 | Train NLL: 3.3696 | Val Loss: 4.5212 | Val NLL: 3.5124\n",
      "Epoch 13 | Train Loss: 4.3483 | Train NLL: 3.3657 | Val Loss: 4.4426 | Val NLL: 3.4861\n",
      "Epoch 14 | Train Loss: 4.2564 | Train NLL: 3.3256 | Val Loss: 4.3779 | Val NLL: 3.4706\n",
      "Epoch 15 | Train Loss: 4.1867 | Train NLL: 3.3019 | Val Loss: 4.3104 | Val NLL: 3.4462\n",
      "Epoch 16 | Train Loss: 4.1274 | Train NLL: 3.2839 | Val Loss: 4.2566 | Val NLL: 3.4320\n",
      "Epoch 17 | Train Loss: 4.0793 | Train NLL: 3.2728 | Val Loss: 4.2026 | Val NLL: 3.4126\n",
      "Epoch 18 | Train Loss: 4.0299 | Train NLL: 3.2565 | Val Loss: 4.1641 | Val NLL: 3.4050\n",
      "Epoch 19 | Train Loss: 3.9930 | Train NLL: 3.2497 | Val Loss: 4.1218 | Val NLL: 3.3925\n",
      "Epoch 20 | Train Loss: 3.9468 | Train NLL: 3.2311 | Val Loss: 4.0738 | Val NLL: 3.3705\n",
      "Epoch 21 | Train Loss: 3.9070 | Train NLL: 3.2171 | Val Loss: 4.0509 | Val NLL: 3.3726\n",
      "Epoch 22 | Train Loss: 3.8871 | Train NLL: 3.2201 | Val Loss: 4.0215 | Val NLL: 3.3648\n",
      "Epoch 23 | Train Loss: 3.8357 | Train NLL: 3.1902 | Val Loss: 3.9903 | Val NLL: 3.3545\n",
      "Epoch 24 | Train Loss: 3.8112 | Train NLL: 3.1862 | Val Loss: 3.9642 | Val NLL: 3.3481\n",
      "Epoch 25 | Train Loss: 3.7899 | Train NLL: 3.1824 | Val Loss: 3.9507 | Val NLL: 3.3513\n",
      "Epoch 26 | Train Loss: 3.7511 | Train NLL: 3.1609 | Val Loss: 3.9166 | Val NLL: 3.3340\n",
      "Epoch 27 | Train Loss: 3.7320 | Train NLL: 3.1576 | Val Loss: 3.8935 | Val NLL: 3.3262\n",
      "Epoch 28 | Train Loss: 3.7117 | Train NLL: 3.1509 | Val Loss: 3.8828 | Val NLL: 3.3279\n",
      "Epoch 29 | Train Loss: 3.6878 | Train NLL: 3.1404 | Val Loss: 3.8537 | Val NLL: 3.3118\n",
      "Epoch 30 | Train Loss: 3.6689 | Train NLL: 3.1339 | Val Loss: 3.8403 | Val NLL: 3.3114\n",
      "Epoch 31 | Train Loss: 3.6470 | Train NLL: 3.1250 | Val Loss: 3.8219 | Val NLL: 3.3061\n",
      "Epoch 32 | Train Loss: 3.6276 | Train NLL: 3.1166 | Val Loss: 3.8125 | Val NLL: 3.3057\n",
      "Epoch 33 | Train Loss: 3.6112 | Train NLL: 3.1102 | Val Loss: 3.8019 | Val NLL: 3.3058\n",
      "Epoch 34 | Train Loss: 3.5980 | Train NLL: 3.1067 | Val Loss: 3.7804 | Val NLL: 3.2938\n",
      "Epoch 35 | Train Loss: 3.5820 | Train NLL: 3.1003 | Val Loss: 3.7728 | Val NLL: 3.2956\n",
      "Epoch 36 | Train Loss: 3.5673 | Train NLL: 3.0941 | Val Loss: 3.7510 | Val NLL: 3.2818\n",
      "Epoch 37 | Train Loss: 3.5483 | Train NLL: 3.0833 | Val Loss: 3.7518 | Val NLL: 3.2906\n",
      "Epoch 38 | Train Loss: 3.5367 | Train NLL: 3.0794 | Val Loss: 3.7322 | Val NLL: 3.2788\n",
      "Epoch 39 | Train Loss: 3.5230 | Train NLL: 3.0732 | Val Loss: 3.7195 | Val NLL: 3.2722\n",
      "Epoch 40 | Train Loss: 3.5143 | Train NLL: 3.0706 | Val Loss: 3.7158 | Val NLL: 3.2757\n",
      "Epoch 41 | Train Loss: 3.5011 | Train NLL: 3.0648 | Val Loss: 3.6998 | Val NLL: 3.2666\n",
      "Epoch 42 | Train Loss: 3.4881 | Train NLL: 3.0579 | Val Loss: 3.6977 | Val NLL: 3.2702\n",
      "Epoch 43 | Train Loss: 3.4735 | Train NLL: 3.0483 | Val Loss: 3.6838 | Val NLL: 3.2601\n",
      "Epoch 44 | Train Loss: 3.4707 | Train NLL: 3.0498 | Val Loss: 3.6806 | Val NLL: 3.2627\n",
      "Epoch 45 | Train Loss: 3.4589 | Train NLL: 3.0426 | Val Loss: 3.6733 | Val NLL: 3.2581\n",
      "Epoch 46 | Train Loss: 3.4463 | Train NLL: 3.0343 | Val Loss: 3.6631 | Val NLL: 3.2526\n",
      "Epoch 47 | Train Loss: 3.4392 | Train NLL: 3.0317 | Val Loss: 3.6571 | Val NLL: 3.2527\n",
      "Epoch 48 | Train Loss: 3.4255 | Train NLL: 3.0227 | Val Loss: 3.6617 | Val NLL: 3.2605\n",
      "Epoch 49 | Train Loss: 3.4167 | Train NLL: 3.0176 | Val Loss: 3.6480 | Val NLL: 3.2508\n",
      "Epoch 50 | Train Loss: 3.4154 | Train NLL: 3.0196 | Val Loss: 3.6454 | Val NLL: 3.2517\n",
      "Epoch 51 | Train Loss: 3.4023 | Train NLL: 3.0088 | Val Loss: 3.6366 | Val NLL: 3.2441\n",
      "Epoch 52 | Train Loss: 3.3965 | Train NLL: 3.0061 | Val Loss: 3.6401 | Val NLL: 3.2518\n",
      "Epoch 53 | Train Loss: 3.3850 | Train NLL: 2.9983 | Val Loss: 3.6427 | Val NLL: 3.2567\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=1e-2, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='overlapping'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0478f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 13.3943 | Train NLL: 5.3226 | Val Loss: 9.7018 | Val NLL: 4.7326\n",
      "Epoch  1 | Train Loss: 8.3886 | Train NLL: 4.4814 | Val Loss: 7.5384 | Val NLL: 4.3448\n",
      "Epoch  2 | Train Loss: 6.9283 | Train NLL: 4.1144 | Val Loss: 6.5572 | Val NLL: 4.0548\n",
      "Epoch  3 | Train Loss: 6.2297 | Train NLL: 3.9423 | Val Loss: 6.0897 | Val NLL: 3.9920\n",
      "Epoch  4 | Train Loss: 5.7478 | Train NLL: 3.7958 | Val Loss: 5.6816 | Val NLL: 3.8617\n",
      "Epoch  5 | Train Loss: 5.4121 | Train NLL: 3.6983 | Val Loss: 5.4051 | Val NLL: 3.7882\n",
      "Epoch  6 | Train Loss: 5.1441 | Train NLL: 3.6092 | Val Loss: 5.1700 | Val NLL: 3.7123\n",
      "Epoch  7 | Train Loss: 4.9706 | Train NLL: 3.5774 | Val Loss: 4.9870 | Val NLL: 3.6551\n",
      "Epoch  8 | Train Loss: 4.8295 | Train NLL: 3.5457 | Val Loss: 4.8283 | Val NLL: 3.5971\n",
      "Epoch  9 | Train Loss: 4.6438 | Train NLL: 3.4579 | Val Loss: 4.7068 | Val NLL: 3.5651\n",
      "Epoch 10 | Train Loss: 4.5225 | Train NLL: 3.4204 | Val Loss: 4.6000 | Val NLL: 3.5344\n",
      "Epoch 11 | Train Loss: 4.4041 | Train NLL: 3.3684 | Val Loss: 4.4966 | Val NLL: 3.4967\n",
      "Epoch 12 | Train Loss: 4.3325 | Train NLL: 3.3614 | Val Loss: 4.4123 | Val NLL: 3.4682\n",
      "Epoch 13 | Train Loss: 4.2536 | Train NLL: 3.3352 | Val Loss: 4.3489 | Val NLL: 3.4549\n",
      "Epoch 14 | Train Loss: 4.1786 | Train NLL: 3.3072 | Val Loss: 4.2857 | Val NLL: 3.4361\n",
      "Epoch 15 | Train Loss: 4.1148 | Train NLL: 3.2866 | Val Loss: 4.2276 | Val NLL: 3.4183\n",
      "Epoch 16 | Train Loss: 4.0577 | Train NLL: 3.2675 | Val Loss: 4.1726 | Val NLL: 3.3999\n",
      "Epoch 17 | Train Loss: 4.0057 | Train NLL: 3.2501 | Val Loss: 4.1273 | Val NLL: 3.3870\n",
      "Epoch 18 | Train Loss: 3.9636 | Train NLL: 3.2383 | Val Loss: 4.0961 | Val NLL: 3.3836\n",
      "Epoch 19 | Train Loss: 3.9435 | Train NLL: 3.2444 | Val Loss: 4.0581 | Val NLL: 3.3713\n",
      "Epoch 20 | Train Loss: 3.8898 | Train NLL: 3.2154 | Val Loss: 4.0301 | Val NLL: 3.3670\n",
      "Epoch 21 | Train Loss: 3.8526 | Train NLL: 3.2015 | Val Loss: 3.9837 | Val NLL: 3.3435\n",
      "Epoch 22 | Train Loss: 3.8215 | Train NLL: 3.1917 | Val Loss: 3.9630 | Val NLL: 3.3421\n",
      "Epoch 23 | Train Loss: 3.7899 | Train NLL: 3.1796 | Val Loss: 3.9478 | Val NLL: 3.3454\n",
      "Epoch 24 | Train Loss: 3.7688 | Train NLL: 3.1762 | Val Loss: 3.9173 | Val NLL: 3.3328\n",
      "Epoch 25 | Train Loss: 3.7347 | Train NLL: 3.1590 | Val Loss: 3.8996 | Val NLL: 3.3318\n",
      "Epoch 26 | Train Loss: 3.7148 | Train NLL: 3.1548 | Val Loss: 3.8633 | Val NLL: 3.3100\n",
      "Epoch 27 | Train Loss: 3.6895 | Train NLL: 3.1424 | Val Loss: 3.8522 | Val NLL: 3.3104\n",
      "Epoch 28 | Train Loss: 3.6728 | Train NLL: 3.1383 | Val Loss: 3.8365 | Val NLL: 3.3064\n",
      "Epoch 29 | Train Loss: 3.6555 | Train NLL: 3.1327 | Val Loss: 3.8226 | Val NLL: 3.3059\n",
      "Epoch 30 | Train Loss: 3.6210 | Train NLL: 3.1110 | Val Loss: 3.8000 | Val NLL: 3.2953\n",
      "Epoch 31 | Train Loss: 3.6184 | Train NLL: 3.1184 | Val Loss: 3.7972 | Val NLL: 3.3008\n",
      "Epoch 32 | Train Loss: 3.5950 | Train NLL: 3.1040 | Val Loss: 3.7806 | Val NLL: 3.2943\n",
      "Epoch 33 | Train Loss: 3.5686 | Train NLL: 3.0878 | Val Loss: 3.7669 | Val NLL: 3.2905\n",
      "Epoch 34 | Train Loss: 3.5582 | Train NLL: 3.0856 | Val Loss: 3.7621 | Val NLL: 3.2931\n",
      "Epoch 35 | Train Loss: 3.5432 | Train NLL: 3.0790 | Val Loss: 3.7510 | Val NLL: 3.2897\n",
      "Epoch 36 | Train Loss: 3.5311 | Train NLL: 3.0741 | Val Loss: 3.7325 | Val NLL: 3.2791\n",
      "Epoch 37 | Train Loss: 3.5136 | Train NLL: 3.0634 | Val Loss: 3.7127 | Val NLL: 3.2655\n",
      "Epoch 38 | Train Loss: 3.4980 | Train NLL: 3.0543 | Val Loss: 3.7143 | Val NLL: 3.2726\n",
      "Epoch 39 | Train Loss: 3.4864 | Train NLL: 3.0483 | Val Loss: 3.7050 | Val NLL: 3.2699\n",
      "Epoch 40 | Train Loss: 3.4759 | Train NLL: 3.0434 | Val Loss: 3.6986 | Val NLL: 3.2679\n",
      "Epoch 41 | Train Loss: 3.4672 | Train NLL: 3.0402 | Val Loss: 3.6911 | Val NLL: 3.2669\n",
      "Epoch 42 | Train Loss: 3.4486 | Train NLL: 3.0272 | Val Loss: 3.6807 | Val NLL: 3.2617\n",
      "Epoch 43 | Train Loss: 3.4380 | Train NLL: 3.0215 | Val Loss: 3.6769 | Val NLL: 3.2621\n",
      "Epoch 44 | Train Loss: 3.4333 | Train NLL: 3.0199 | Val Loss: 3.6672 | Val NLL: 3.2549\n",
      "Epoch 45 | Train Loss: 3.4235 | Train NLL: 3.0130 | Val Loss: 3.6699 | Val NLL: 3.2599\n",
      "Epoch 46 | Train Loss: 3.4108 | Train NLL: 3.0041 | Val Loss: 3.6640 | Val NLL: 3.2597\n",
      "Epoch 47 | Train Loss: 3.4030 | Train NLL: 3.0003 | Val Loss: 3.6547 | Val NLL: 3.2532\n",
      "Epoch 48 | Train Loss: 3.3910 | Train NLL: 2.9912 | Val Loss: 3.6652 | Val NLL: 3.2638\n",
      "Epoch 49 | Train Loss: 3.3947 | Train NLL: 2.9963 | Val Loss: 3.6429 | Val NLL: 3.2461\n",
      "Epoch 50 | Train Loss: 3.3729 | Train NLL: 2.9785 | Val Loss: 3.6461 | Val NLL: 3.2529\n",
      "Epoch 51 | Train Loss: 3.3784 | Train NLL: 2.9861 | Val Loss: 3.6425 | Val NLL: 3.2508\n",
      "Epoch 52 | Train Loss: 3.3596 | Train NLL: 2.9688 | Val Loss: 3.6406 | Val NLL: 3.2500\n",
      "Epoch 53 | Train Loss: 3.3520 | Train NLL: 2.9635 | Val Loss: 3.6450 | Val NLL: 3.2569\n",
      "Epoch 54 | Train Loss: 3.3466 | Train NLL: 2.9604 | Val Loss: 3.6460 | Val NLL: 3.2605\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=3e-3, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='overlapping'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0f98035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 13.9161 | Train NLL: 5.4364 | Val Loss: 10.4129 | Val NLL: 4.8509\n",
      "Epoch  1 | Train Loss: 9.0485 | Train NLL: 4.5726 | Val Loss: 8.1808 | Val NLL: 4.4649\n",
      "Epoch  2 | Train Loss: 7.4977 | Train NLL: 4.2056 | Val Loss: 7.1225 | Val NLL: 4.1840\n",
      "Epoch  3 | Train Loss: 6.6908 | Train NLL: 4.0012 | Val Loss: 6.4677 | Val NLL: 4.0051\n",
      "Epoch  4 | Train Loss: 6.1257 | Train NLL: 3.8352 | Val Loss: 6.0140 | Val NLL: 3.8871\n",
      "Epoch  5 | Train Loss: 5.7132 | Train NLL: 3.7203 | Val Loss: 5.7076 | Val NLL: 3.8381\n",
      "Epoch  6 | Train Loss: 5.4285 | Train NLL: 3.6624 | Val Loss: 5.4449 | Val NLL: 3.7765\n",
      "Epoch  7 | Train Loss: 5.1815 | Train NLL: 3.5933 | Val Loss: 5.2046 | Val NLL: 3.6956\n",
      "Epoch  8 | Train Loss: 4.9674 | Train NLL: 3.5270 | Val Loss: 5.0116 | Val NLL: 3.6363\n",
      "Epoch  9 | Train Loss: 4.8190 | Train NLL: 3.5000 | Val Loss: 4.8746 | Val NLL: 3.6084\n",
      "Epoch 10 | Train Loss: 4.6829 | Train NLL: 3.4638 | Val Loss: 4.7451 | Val NLL: 3.5715\n",
      "Epoch 11 | Train Loss: 4.5551 | Train NLL: 3.4226 | Val Loss: 4.6426 | Val NLL: 3.5487\n",
      "Epoch 12 | Train Loss: 4.4471 | Train NLL: 3.3879 | Val Loss: 4.5481 | Val NLL: 3.5221\n",
      "Epoch 13 | Train Loss: 4.3573 | Train NLL: 3.3608 | Val Loss: 4.4703 | Val NLL: 3.5027\n",
      "Epoch 14 | Train Loss: 4.2998 | Train NLL: 3.3575 | Val Loss: 4.3971 | Val NLL: 3.4804\n",
      "Epoch 15 | Train Loss: 4.2099 | Train NLL: 3.3169 | Val Loss: 4.3370 | Val NLL: 3.4671\n",
      "Epoch 16 | Train Loss: 4.1503 | Train NLL: 3.3012 | Val Loss: 4.2759 | Val NLL: 3.4459\n",
      "Epoch 17 | Train Loss: 4.0947 | Train NLL: 3.2838 | Val Loss: 4.2494 | Val NLL: 3.4563\n",
      "Epoch 18 | Train Loss: 4.0564 | Train NLL: 3.2796 | Val Loss: 4.1791 | Val NLL: 3.4182\n",
      "Epoch 19 | Train Loss: 3.9945 | Train NLL: 3.2491 | Val Loss: 4.1298 | Val NLL: 3.3990\n",
      "Epoch 20 | Train Loss: 3.9511 | Train NLL: 3.2343 | Val Loss: 4.0929 | Val NLL: 3.3887\n",
      "Epoch 21 | Train Loss: 3.9339 | Train NLL: 3.2418 | Val Loss: 4.0878 | Val NLL: 3.4053\n",
      "Epoch 22 | Train Loss: 3.8864 | Train NLL: 3.2159 | Val Loss: 4.0364 | Val NLL: 3.3777\n",
      "Epoch 23 | Train Loss: 3.8387 | Train NLL: 3.1916 | Val Loss: 3.9954 | Val NLL: 3.3576\n",
      "Epoch 24 | Train Loss: 3.8150 | Train NLL: 3.1878 | Val Loss: 3.9679 | Val NLL: 3.3505\n",
      "Epoch 25 | Train Loss: 3.7896 | Train NLL: 3.1813 | Val Loss: 3.9643 | Val NLL: 3.3639\n",
      "Epoch 26 | Train Loss: 3.7645 | Train NLL: 3.1724 | Val Loss: 3.9411 | Val NLL: 3.3550\n",
      "Epoch 27 | Train Loss: 3.7350 | Train NLL: 3.1567 | Val Loss: 3.9102 | Val NLL: 3.3389\n",
      "Epoch 28 | Train Loss: 3.7138 | Train NLL: 3.1505 | Val Loss: 3.8853 | Val NLL: 3.3288\n",
      "Epoch 29 | Train Loss: 3.6844 | Train NLL: 3.1348 | Val Loss: 3.8679 | Val NLL: 3.3229\n",
      "Epoch 30 | Train Loss: 3.6600 | Train NLL: 3.1214 | Val Loss: 3.8531 | Val NLL: 3.3211\n",
      "Epoch 31 | Train Loss: 3.6416 | Train NLL: 3.1158 | Val Loss: 3.8310 | Val NLL: 3.3096\n",
      "Epoch 32 | Train Loss: 3.6261 | Train NLL: 3.1105 | Val Loss: 3.8242 | Val NLL: 3.3138\n",
      "Epoch 33 | Train Loss: 3.6093 | Train NLL: 3.1039 | Val Loss: 3.8258 | Val NLL: 3.3247\n",
      "Epoch 34 | Train Loss: 3.5966 | Train NLL: 3.0997 | Val Loss: 3.7959 | Val NLL: 3.3028\n",
      "Epoch 35 | Train Loss: 3.5736 | Train NLL: 3.0854 | Val Loss: 3.7851 | Val NLL: 3.3008\n",
      "Epoch 36 | Train Loss: 3.5616 | Train NLL: 3.0819 | Val Loss: 3.7936 | Val NLL: 3.3173\n",
      "Epoch 37 | Train Loss: 3.5455 | Train NLL: 3.0737 | Val Loss: 3.7808 | Val NLL: 3.3126\n",
      "Epoch 38 | Train Loss: 3.5341 | Train NLL: 3.0686 | Val Loss: 3.7525 | Val NLL: 3.2899\n",
      "Epoch 39 | Train Loss: 3.5242 | Train NLL: 3.0649 | Val Loss: 3.7551 | Val NLL: 3.2988\n",
      "Epoch 40 | Train Loss: 3.5047 | Train NLL: 3.0512 | Val Loss: 3.7292 | Val NLL: 3.2785\n",
      "Epoch 41 | Train Loss: 3.4921 | Train NLL: 3.0450 | Val Loss: 3.7361 | Val NLL: 3.2923\n",
      "Epoch 42 | Train Loss: 3.4776 | Train NLL: 3.0363 | Val Loss: 3.7315 | Val NLL: 3.2930\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=3e-2, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='overlapping'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "570afe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 11.9217 | Train NLL: 5.2205 | Val Loss: 8.8664 | Val NLL: 4.7848\n",
      "Epoch  1 | Train Loss: 7.5037 | Train NLL: 4.4213 | Val Loss: 6.7259 | Val NLL: 4.3049\n",
      "Epoch  2 | Train Loss: 6.2385 | Train NLL: 4.1309 | Val Loss: 6.0041 | Val NLL: 4.1385\n",
      "Epoch  3 | Train Loss: 5.6154 | Train NLL: 3.9042 | Val Loss: 5.5102 | Val NLL: 3.9311\n",
      "Epoch  4 | Train Loss: 5.2325 | Train NLL: 3.7517 | Val Loss: 5.2494 | Val NLL: 3.8578\n",
      "Epoch  5 | Train Loss: 4.9843 | Train NLL: 3.6632 | Val Loss: 5.0203 | Val NLL: 3.7645\n",
      "Epoch  6 | Train Loss: 4.7796 | Train NLL: 3.5776 | Val Loss: 4.8142 | Val NLL: 3.6633\n",
      "Epoch  7 | Train Loss: 4.6178 | Train NLL: 3.5100 | Val Loss: 4.6760 | Val NLL: 3.6094\n",
      "Epoch  8 | Train Loss: 4.4777 | Train NLL: 3.4473 | Val Loss: 4.5649 | Val NLL: 3.5696\n",
      "Epoch  9 | Train Loss: 4.3793 | Train NLL: 3.4138 | Val Loss: 4.4763 | Val NLL: 3.5398\n",
      "Epoch 10 | Train Loss: 4.2749 | Train NLL: 3.3643 | Val Loss: 4.3909 | Val NLL: 3.5064\n",
      "Epoch 11 | Train Loss: 4.1945 | Train NLL: 3.3325 | Val Loss: 4.3139 | Val NLL: 3.4744\n",
      "Epoch 12 | Train Loss: 4.1272 | Train NLL: 3.3077 | Val Loss: 4.2546 | Val NLL: 3.4551\n",
      "Epoch 13 | Train Loss: 4.0749 | Train NLL: 3.2922 | Val Loss: 4.2036 | Val NLL: 3.4383\n",
      "Epoch 14 | Train Loss: 4.0144 | Train NLL: 3.2648 | Val Loss: 4.1520 | Val NLL: 3.4183\n",
      "Epoch 15 | Train Loss: 3.9636 | Train NLL: 3.2438 | Val Loss: 4.1045 | Val NLL: 3.3987\n",
      "Epoch 16 | Train Loss: 3.9259 | Train NLL: 3.2333 | Val Loss: 4.0728 | Val NLL: 3.3931\n",
      "Epoch 17 | Train Loss: 3.8808 | Train NLL: 3.2131 | Val Loss: 4.0322 | Val NLL: 3.3759\n",
      "Epoch 18 | Train Loss: 3.8447 | Train NLL: 3.1993 | Val Loss: 3.9975 | Val NLL: 3.3628\n",
      "Epoch 19 | Train Loss: 3.8138 | Train NLL: 3.1890 | Val Loss: 3.9732 | Val NLL: 3.3580\n",
      "Epoch 20 | Train Loss: 3.7749 | Train NLL: 3.1692 | Val Loss: 3.9433 | Val NLL: 3.3470\n",
      "Epoch 21 | Train Loss: 3.7482 | Train NLL: 3.1609 | Val Loss: 3.9250 | Val NLL: 3.3461\n",
      "Epoch 22 | Train Loss: 3.7178 | Train NLL: 3.1472 | Val Loss: 3.8967 | Val NLL: 3.3339\n",
      "Epoch 23 | Train Loss: 3.7028 | Train NLL: 3.1474 | Val Loss: 3.8719 | Val NLL: 3.3232\n",
      "Epoch 24 | Train Loss: 3.6728 | Train NLL: 3.1311 | Val Loss: 3.8627 | Val NLL: 3.3272\n",
      "Epoch 25 | Train Loss: 3.6470 | Train NLL: 3.1185 | Val Loss: 3.8300 | Val NLL: 3.3075\n",
      "Epoch 26 | Train Loss: 3.6267 | Train NLL: 3.1107 | Val Loss: 3.8170 | Val NLL: 3.3062\n",
      "Epoch 27 | Train Loss: 3.6062 | Train NLL: 3.1012 | Val Loss: 3.7965 | Val NLL: 3.2970\n",
      "Epoch 28 | Train Loss: 3.5840 | Train NLL: 3.0902 | Val Loss: 3.7834 | Val NLL: 3.2944\n",
      "Epoch 29 | Train Loss: 3.5733 | Train NLL: 3.0888 | Val Loss: 3.7738 | Val NLL: 3.2933\n",
      "Epoch 30 | Train Loss: 3.5483 | Train NLL: 3.0736 | Val Loss: 3.7601 | Val NLL: 3.2896\n",
      "Epoch 31 | Train Loss: 3.5360 | Train NLL: 3.0700 | Val Loss: 3.7463 | Val NLL: 3.2835\n",
      "Epoch 32 | Train Loss: 3.5217 | Train NLL: 3.0637 | Val Loss: 3.7319 | Val NLL: 3.2773\n",
      "Epoch 33 | Train Loss: 3.5099 | Train NLL: 3.0592 | Val Loss: 3.7197 | Val NLL: 3.2718\n",
      "Epoch 34 | Train Loss: 3.4917 | Train NLL: 3.0481 | Val Loss: 3.7121 | Val NLL: 3.2717\n",
      "Epoch 35 | Train Loss: 3.4873 | Train NLL: 3.0506 | Val Loss: 3.6983 | Val NLL: 3.2639\n",
      "Epoch 36 | Train Loss: 3.4694 | Train NLL: 3.0389 | Val Loss: 3.6946 | Val NLL: 3.2667\n",
      "Epoch 37 | Train Loss: 3.4581 | Train NLL: 3.0331 | Val Loss: 3.7010 | Val NLL: 3.2781\n",
      "Epoch 38 | Train Loss: 3.4522 | Train NLL: 3.0328 | Val Loss: 3.6800 | Val NLL: 3.2624\n",
      "Epoch 39 | Train Loss: 3.4381 | Train NLL: 3.0235 | Val Loss: 3.6735 | Val NLL: 3.2602\n",
      "Epoch 40 | Train Loss: 3.4372 | Train NLL: 3.0268 | Val Loss: 3.6603 | Val NLL: 3.2514\n",
      "Epoch 41 | Train Loss: 3.4185 | Train NLL: 3.0126 | Val Loss: 3.6589 | Val NLL: 3.2547\n",
      "Epoch 42 | Train Loss: 3.4099 | Train NLL: 3.0085 | Val Loss: 3.6461 | Val NLL: 3.2463\n",
      "Epoch 43 | Train Loss: 3.4017 | Train NLL: 3.0043 | Val Loss: 3.6455 | Val NLL: 3.2485\n",
      "Epoch 44 | Train Loss: 3.3918 | Train NLL: 2.9979 | Val Loss: 3.6411 | Val NLL: 3.2483\n",
      "Epoch 45 | Train Loss: 3.3789 | Train NLL: 2.9881 | Val Loss: 3.6429 | Val NLL: 3.2521\n",
      "Epoch 46 | Train Loss: 3.3729 | Train NLL: 2.9850 | Val Loss: 3.6288 | Val NLL: 3.2421\n",
      "Epoch 47 | Train Loss: 3.3595 | Train NLL: 2.9745 | Val Loss: 3.6275 | Val NLL: 3.2426\n",
      "Epoch 48 | Train Loss: 3.3593 | Train NLL: 2.9769 | Val Loss: 3.6235 | Val NLL: 3.2418\n",
      "Epoch 49 | Train Loss: 3.3581 | Train NLL: 2.9786 | Val Loss: 3.6296 | Val NLL: 3.2501\n",
      "Epoch 50 | Train Loss: 3.3431 | Train NLL: 2.9655 | Val Loss: 3.6252 | Val NLL: 3.2485\n"
     ]
    }
   ],
   "source": [
    "model = NMC(n_bins=N_BINS, importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='overlapping'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b15605ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 10.0437 | Train NLL: 5.0811 | Val Loss: 8.2553 | Val NLL: 4.7119\n",
      "Epoch  1 | Train Loss: 7.3686 | Train NLL: 4.4335 | Val Loss: 6.8535 | Val NLL: 4.3595\n",
      "Epoch  2 | Train Loss: 6.3810 | Train NLL: 4.1335 | Val Loss: 6.2146 | Val NLL: 4.1711\n",
      "Epoch  3 | Train Loss: 5.8812 | Train NLL: 3.9796 | Val Loss: 5.8254 | Val NLL: 4.0511\n",
      "Epoch  4 | Train Loss: 5.4626 | Train NLL: 3.7885 | Val Loss: 5.4715 | Val NLL: 3.8910\n",
      "Epoch  5 | Train Loss: 5.2162 | Train NLL: 3.7109 | Val Loss: 5.2800 | Val NLL: 3.8460\n",
      "Epoch  6 | Train Loss: 5.0107 | Train NLL: 3.6361 | Val Loss: 5.0315 | Val NLL: 3.7143\n",
      "Epoch  7 | Train Loss: 4.8294 | Train NLL: 3.5609 | Val Loss: 4.8637 | Val NLL: 3.6432\n",
      "Epoch  8 | Train Loss: 4.6837 | Train NLL: 3.5034 | Val Loss: 4.7289 | Val NLL: 3.5878\n",
      "Epoch  9 | Train Loss: 4.5716 | Train NLL: 3.4649 | Val Loss: 4.6366 | Val NLL: 3.5639\n",
      "Epoch 10 | Train Loss: 4.4622 | Train NLL: 3.4199 | Val Loss: 4.5277 | Val NLL: 3.5158\n",
      "Epoch 11 | Train Loss: 4.3788 | Train NLL: 3.3929 | Val Loss: 4.4536 | Val NLL: 3.4938\n",
      "Epoch 12 | Train Loss: 4.2850 | Train NLL: 3.3499 | Val Loss: 4.3795 | Val NLL: 3.4688\n",
      "Epoch 13 | Train Loss: 4.2092 | Train NLL: 3.3197 | Val Loss: 4.3206 | Val NLL: 3.4520\n",
      "Epoch 14 | Train Loss: 4.1417 | Train NLL: 3.2932 | Val Loss: 4.2625 | Val NLL: 3.4333\n",
      "Epoch 15 | Train Loss: 4.0915 | Train NLL: 3.2802 | Val Loss: 4.2121 | Val NLL: 3.4179\n",
      "Epoch 16 | Train Loss: 4.0517 | Train NLL: 3.2739 | Val Loss: 4.1669 | Val NLL: 3.4043\n",
      "Epoch 17 | Train Loss: 4.0160 | Train NLL: 3.2681 | Val Loss: 4.1214 | Val NLL: 3.3871\n",
      "Epoch 18 | Train Loss: 3.9687 | Train NLL: 3.2481 | Val Loss: 4.0822 | Val NLL: 3.3744\n",
      "Epoch 19 | Train Loss: 3.9177 | Train NLL: 3.2229 | Val Loss: 4.0558 | Val NLL: 3.3734\n",
      "Epoch 20 | Train Loss: 3.8825 | Train NLL: 3.2124 | Val Loss: 4.0233 | Val NLL: 3.3643\n",
      "Epoch 21 | Train Loss: 3.8532 | Train NLL: 3.2052 | Val Loss: 3.9963 | Val NLL: 3.3580\n",
      "Epoch 22 | Train Loss: 3.8221 | Train NLL: 3.1943 | Val Loss: 3.9702 | Val NLL: 3.3509\n",
      "Epoch 23 | Train Loss: 3.7939 | Train NLL: 3.1847 | Val Loss: 3.9572 | Val NLL: 3.3558\n",
      "Epoch 24 | Train Loss: 3.7546 | Train NLL: 3.1626 | Val Loss: 3.9091 | Val NLL: 3.3250\n",
      "Epoch 25 | Train Loss: 3.7341 | Train NLL: 3.1587 | Val Loss: 3.8973 | Val NLL: 3.3287\n",
      "Epoch 26 | Train Loss: 3.7066 | Train NLL: 3.1463 | Val Loss: 3.8763 | Val NLL: 3.3223\n",
      "Epoch 27 | Train Loss: 3.6793 | Train NLL: 3.1330 | Val Loss: 3.8391 | Val NLL: 3.2990\n",
      "Epoch 28 | Train Loss: 3.6698 | Train NLL: 3.1366 | Val Loss: 3.8325 | Val NLL: 3.3045\n",
      "Epoch 29 | Train Loss: 3.6416 | Train NLL: 3.1206 | Val Loss: 3.8117 | Val NLL: 3.2959\n",
      "Epoch 30 | Train Loss: 3.6192 | Train NLL: 3.1098 | Val Loss: 3.7968 | Val NLL: 3.2922\n",
      "Epoch 31 | Train Loss: 3.6161 | Train NLL: 3.1171 | Val Loss: 3.7812 | Val NLL: 3.2854\n",
      "Epoch 32 | Train Loss: 3.5932 | Train NLL: 3.1038 | Val Loss: 3.7646 | Val NLL: 3.2796\n",
      "Epoch 33 | Train Loss: 3.5691 | Train NLL: 3.0897 | Val Loss: 3.7430 | Val NLL: 3.2676\n",
      "Epoch 34 | Train Loss: 3.5617 | Train NLL: 3.0912 | Val Loss: 3.7388 | Val NLL: 3.2710\n",
      "Epoch 35 | Train Loss: 3.5390 | Train NLL: 3.0763 | Val Loss: 3.7180 | Val NLL: 3.2591\n",
      "Epoch 36 | Train Loss: 3.5260 | Train NLL: 3.0716 | Val Loss: 3.7142 | Val NLL: 3.2621\n",
      "Epoch 37 | Train Loss: 3.5135 | Train NLL: 3.0660 | Val Loss: 3.7060 | Val NLL: 3.2606\n",
      "Epoch 38 | Train Loss: 3.4973 | Train NLL: 3.0565 | Val Loss: 3.6985 | Val NLL: 3.2602\n",
      "Epoch 39 | Train Loss: 3.4884 | Train NLL: 3.0544 | Val Loss: 3.6932 | Val NLL: 3.2618\n",
      "Epoch 40 | Train Loss: 3.4734 | Train NLL: 3.0456 | Val Loss: 3.6782 | Val NLL: 3.2524\n",
      "Epoch 41 | Train Loss: 3.4625 | Train NLL: 3.0400 | Val Loss: 3.6701 | Val NLL: 3.2498\n",
      "Epoch 42 | Train Loss: 3.4571 | Train NLL: 3.0401 | Val Loss: 3.6746 | Val NLL: 3.2590\n",
      "Epoch 43 | Train Loss: 3.4410 | Train NLL: 3.0286 | Val Loss: 3.6588 | Val NLL: 3.2475\n",
      "Epoch 44 | Train Loss: 3.4352 | Train NLL: 3.0271 | Val Loss: 3.6540 | Val NLL: 3.2469\n",
      "Epoch 45 | Train Loss: 3.4216 | Train NLL: 3.0177 | Val Loss: 3.6527 | Val NLL: 3.2501\n",
      "Epoch 46 | Train Loss: 3.4150 | Train NLL: 3.0150 | Val Loss: 3.6510 | Val NLL: 3.2528\n",
      "Epoch 47 | Train Loss: 3.4108 | Train NLL: 3.0150 | Val Loss: 3.6412 | Val NLL: 3.2458\n",
      "Epoch 48 | Train Loss: 3.3966 | Train NLL: 3.0038 | Val Loss: 3.6388 | Val NLL: 3.2476\n",
      "Epoch 49 | Train Loss: 3.3864 | Train NLL: 2.9976 | Val Loss: 3.6351 | Val NLL: 3.2474\n",
      "Epoch 50 | Train Loss: 3.3820 | Train NLL: 2.9954 | Val Loss: 3.6241 | Val NLL: 3.2376\n",
      "Epoch 51 | Train Loss: 3.3743 | Train NLL: 2.9906 | Val Loss: 3.6178 | Val NLL: 3.2351\n",
      "Epoch 52 | Train Loss: 3.3656 | Train NLL: 2.9851 | Val Loss: 3.6294 | Val NLL: 3.2493\n",
      "Epoch 53 | Train Loss: 3.3616 | Train NLL: 2.9834 | Val Loss: 3.6268 | Val NLL: 3.2480\n"
     ]
    }
   ],
   "source": [
    "model = NSurv(n_bins=N_BINS, importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='overlapping'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "194ee09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 3.6507 | Train NLL: 1.3185 | Val Loss: 2.6634 | Val NLL: 1.1265\n",
      "Epoch  1 | Train Loss: 2.3147 | Train NLL: 1.1033 | Val Loss: 2.0202 | Val NLL: 1.0370\n",
      "Epoch  2 | Train Loss: 1.9211 | Train NLL: 1.0630 | Val Loss: 1.7862 | Val NLL: 1.0300\n",
      "Epoch  3 | Train Loss: 1.7282 | Train NLL: 1.0430 | Val Loss: 1.6437 | Val NLL: 1.0208\n",
      "Epoch  4 | Train Loss: 1.6012 | Train NLL: 1.0263 | Val Loss: 1.5365 | Val NLL: 1.0056\n",
      "Epoch  5 | Train Loss: 1.5149 | Train NLL: 1.0192 | Val Loss: 1.4582 | Val NLL: 0.9955\n",
      "Epoch  6 | Train Loss: 1.4493 | Train NLL: 1.0135 | Val Loss: 1.3977 | Val NLL: 0.9877\n",
      "Epoch  7 | Train Loss: 1.3971 | Train NLL: 1.0084 | Val Loss: 1.3497 | Val NLL: 0.9816\n",
      "Epoch  8 | Train Loss: 1.3550 | Train NLL: 1.0046 | Val Loss: 1.3109 | Val NLL: 0.9777\n",
      "Epoch  9 | Train Loss: 1.3202 | Train NLL: 1.0014 | Val Loss: 1.2793 | Val NLL: 0.9746\n",
      "Epoch 10 | Train Loss: 1.2895 | Train NLL: 0.9972 | Val Loss: 1.2516 | Val NLL: 0.9716\n",
      "Epoch 11 | Train Loss: 1.2647 | Train NLL: 0.9954 | Val Loss: 1.2290 | Val NLL: 0.9704\n",
      "Epoch 12 | Train Loss: 1.2414 | Train NLL: 0.9919 | Val Loss: 1.2093 | Val NLL: 0.9690\n",
      "Epoch 13 | Train Loss: 1.2219 | Train NLL: 0.9894 | Val Loss: 1.1922 | Val NLL: 0.9678\n",
      "Epoch 14 | Train Loss: 1.2051 | Train NLL: 0.9877 | Val Loss: 1.1751 | Val NLL: 0.9652\n",
      "Epoch 15 | Train Loss: 1.1892 | Train NLL: 0.9854 | Val Loss: 1.1632 | Val NLL: 0.9659\n",
      "Epoch 16 | Train Loss: 1.1744 | Train NLL: 0.9827 | Val Loss: 1.1499 | Val NLL: 0.9641\n",
      "Epoch 17 | Train Loss: 1.1620 | Train NLL: 0.9812 | Val Loss: 1.1385 | Val NLL: 0.9630\n",
      "Epoch 18 | Train Loss: 1.1514 | Train NLL: 0.9801 | Val Loss: 1.1283 | Val NLL: 0.9616\n",
      "Epoch 19 | Train Loss: 1.1401 | Train NLL: 0.9775 | Val Loss: 1.1188 | Val NLL: 0.9604\n",
      "Epoch 20 | Train Loss: 1.1304 | Train NLL: 0.9755 | Val Loss: 1.1105 | Val NLL: 0.9595\n",
      "Epoch 21 | Train Loss: 1.1230 | Train NLL: 0.9755 | Val Loss: 1.1028 | Val NLL: 0.9590\n",
      "Epoch 22 | Train Loss: 1.1151 | Train NLL: 0.9741 | Val Loss: 1.0957 | Val NLL: 0.9580\n",
      "Epoch 23 | Train Loss: 1.1068 | Train NLL: 0.9720 | Val Loss: 1.0894 | Val NLL: 0.9578\n",
      "Epoch 24 | Train Loss: 1.0998 | Train NLL: 0.9705 | Val Loss: 1.0829 | Val NLL: 0.9565\n",
      "Epoch 25 | Train Loss: 1.0936 | Train NLL: 0.9694 | Val Loss: 1.0767 | Val NLL: 0.9554\n",
      "Epoch 26 | Train Loss: 1.0872 | Train NLL: 0.9678 | Val Loss: 1.0721 | Val NLL: 0.9551\n",
      "Epoch 27 | Train Loss: 1.0824 | Train NLL: 0.9671 | Val Loss: 1.0674 | Val NLL: 0.9544\n",
      "Epoch 28 | Train Loss: 1.0769 | Train NLL: 0.9656 | Val Loss: 1.0629 | Val NLL: 0.9539\n",
      "Epoch 29 | Train Loss: 1.0706 | Train NLL: 0.9627 | Val Loss: 1.0597 | Val NLL: 0.9538\n",
      "Epoch 30 | Train Loss: 1.0665 | Train NLL: 0.9619 | Val Loss: 1.0550 | Val NLL: 0.9525\n",
      "Epoch 31 | Train Loss: 1.0626 | Train NLL: 0.9613 | Val Loss: 1.0520 | Val NLL: 0.9525\n",
      "Epoch 32 | Train Loss: 1.0580 | Train NLL: 0.9596 | Val Loss: 1.0518 | Val NLL: 0.9547\n",
      "Epoch 33 | Train Loss: 1.0551 | Train NLL: 0.9592 | Val Loss: 1.0456 | Val NLL: 0.9514\n",
      "Epoch 34 | Train Loss: 1.0509 | Train NLL: 0.9571 | Val Loss: 1.0459 | Val NLL: 0.9533\n",
      "Epoch 35 | Train Loss: 1.0471 | Train NLL: 0.9552 | Val Loss: 1.0414 | Val NLL: 0.9513\n",
      "Epoch 36 | Train Loss: 1.0430 | Train NLL: 0.9537 | Val Loss: 1.0388 | Val NLL: 0.9510\n",
      "Epoch 37 | Train Loss: 1.0400 | Train NLL: 0.9528 | Val Loss: 1.0372 | Val NLL: 0.9511\n",
      "Epoch 38 | Train Loss: 1.0395 | Train NLL: 0.9535 | Val Loss: 1.0363 | Val NLL: 0.9515\n",
      "Epoch 39 | Train Loss: 1.0356 | Train NLL: 0.9514 | Val Loss: 1.0329 | Val NLL: 0.9498\n",
      "Epoch 40 | Train Loss: 1.0320 | Train NLL: 0.9493 | Val Loss: 1.0310 | Val NLL: 0.9494\n",
      "Epoch 41 | Train Loss: 1.0300 | Train NLL: 0.9486 | Val Loss: 1.0301 | Val NLL: 0.9498\n",
      "Epoch 42 | Train Loss: 1.0271 | Train NLL: 0.9473 | Val Loss: 1.0290 | Val NLL: 0.9501\n",
      "Epoch 43 | Train Loss: 1.0248 | Train NLL: 0.9461 | Val Loss: 1.0278 | Val NLL: 0.9501\n",
      "Epoch 44 | Train Loss: 1.0227 | Train NLL: 0.9451 | Val Loss: 1.0277 | Val NLL: 0.9509\n",
      "Epoch 45 | Train Loss: 1.0204 | Train NLL: 0.9435 | Val Loss: 1.0275 | Val NLL: 0.9511\n",
      "Epoch 46 | Train Loss: 1.0189 | Train NLL: 0.9428 | Val Loss: 1.0282 | Val NLL: 0.9530\n",
      "Epoch 47 | Train Loss: 1.0168 | Train NLL: 0.9418 | Val Loss: 1.0260 | Val NLL: 0.9517\n",
      "Epoch 48 | Train Loss: 1.0153 | Train NLL: 0.9408 | Val Loss: 1.0296 | Val NLL: 0.9559\n",
      "Epoch 49 | Train Loss: 1.0137 | Train NLL: 0.9399 | Val Loss: 1.0294 | Val NLL: 0.9558\n"
     ]
    }
   ],
   "source": [
    "model = MLP(importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='overlapping'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee215d",
   "metadata": {},
   "source": [
    "## Dependent Censoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "64f4724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(11)\n",
    "\n",
    "feature_order = rs.permutation(len(X.T))\n",
    "T_features = feature_order[N_OVERLAPPING:(N_FEATURES + N_OVERLAPPING)]\n",
    "E_features = feature_order[N_FEATURES:(2 * N_FEATURES)]\n",
    "C_features = feature_order[(2 * N_FEATURES - N_OVERLAPPING):(3 * N_FEATURES - N_OVERLAPPING)]\n",
    "\n",
    "T_weights = rs.randn(len(T_features))\n",
    "E_weights = rs.randn(len(E_features))\n",
    "C_weights = rs.randn(len(C_features))\n",
    "\n",
    "T = X[:, T_features] @ T_weights\n",
    "E = X[:, E_features] @ E_weights\n",
    "C = X[:, C_features] @ C_weights\n",
    "\n",
    "t_cont = np.exp((T - np.mean(T)) / (2 * np.std(T)))\n",
    "c_cont = np.exp((C - np.mean(C)) / (2 * np.std(C)))\n",
    "\n",
    "e_prob = 1 - 1 / (1 + np.exp(4. * (E - E.mean()) / E.std()))\n",
    "e = (rs.rand(len(e_prob)) < e_prob).astype(int)\n",
    "\n",
    "ct_cont = np.stack([c_cont, t_cont]).T\n",
    "y_cont = e * np.amin(ct_cont, axis=1) + (1 - e) * c_cont\n",
    "\n",
    "s = e * np.argmin(ct_cont, axis=1)\n",
    "y_disc, t_disc, c_disc = discretize(y_cont, t_cont, c_cont, nbins=N_BINS)\n",
    "\n",
    "y = onehot(y_disc, ncategories=N_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a4deea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pstrue = s_train.mean()\n",
    "importance_weights = [1 / (1 - pstrue), 1 / pstrue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "35e70aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many are susceptible?\n",
      "0    4196\n",
      "1    4909\n",
      "dtype: int64\n",
      "How many are observed?\n",
      "0    6558\n",
      "1    2547\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('How many are susceptible?')\n",
    "print(pd.value_counts(e).sort_index())\n",
    "\n",
    "print('How many are observed?')\n",
    "print(pd.value_counts(s).sort_index())\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))\n",
    "\n",
    "ax[0].hist(e_prob)\n",
    "ax[1].hist(t_disc)\n",
    "ax[2].hist(c_disc)\n",
    "ax[3].hist(y_disc)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "69e11593",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, s_train = X[:6000], y[:6000], s[:6000]\n",
    "x_val, y_val, s_val = X[6000:7500], y[6000:7500], s[6000:7500]\n",
    "x_test, y_test, s_test, e_test = X[7500:], y[7500:], s[7500:], e[7500:]\n",
    "t_disc_test, c_disc_test, y_disc_test = t_disc[7500:], c_disc[7500:], y_disc[7500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e818842b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 13.4417 | Train NLL: 4.6773 | Val Loss: 9.8124 | Val NLL: 3.8406\n",
      "Epoch  1 | Train Loss: 8.3048 | Train NLL: 3.3949 | Val Loss: 7.4583 | Val NLL: 3.3050\n",
      "Epoch  2 | Train Loss: 6.5449 | Train NLL: 2.8273 | Val Loss: 6.1192 | Val NLL: 2.7628\n",
      "Epoch  3 | Train Loss: 5.5746 | Train NLL: 2.4737 | Val Loss: 5.4387 | Val NLL: 2.5554\n",
      "Epoch  4 | Train Loss: 4.9163 | Train NLL: 2.2161 | Val Loss: 4.8640 | Val NLL: 2.3144\n",
      "Epoch  5 | Train Loss: 4.4663 | Train NLL: 2.0515 | Val Loss: 4.4399 | Val NLL: 2.1568\n",
      "Epoch  6 | Train Loss: 4.0730 | Train NLL: 1.9007 | Val Loss: 4.0534 | Val NLL: 1.9795\n",
      "Epoch  7 | Train Loss: 3.7560 | Train NLL: 1.7707 | Val Loss: 3.7982 | Val NLL: 1.8927\n",
      "Epoch  8 | Train Loss: 3.5288 | Train NLL: 1.6939 | Val Loss: 3.5376 | Val NLL: 1.7648\n",
      "Epoch  9 | Train Loss: 3.3281 | Train NLL: 1.6153 | Val Loss: 3.3718 | Val NLL: 1.7116\n",
      "Epoch 10 | Train Loss: 3.1339 | Train NLL: 1.5257 | Val Loss: 3.1562 | Val NLL: 1.5958\n",
      "Epoch 11 | Train Loss: 2.9908 | Train NLL: 1.4758 | Val Loss: 3.0459 | Val NLL: 1.5717\n",
      "Epoch 12 | Train Loss: 2.8474 | Train NLL: 1.4155 | Val Loss: 2.9058 | Val NLL: 1.5132\n",
      "Epoch 13 | Train Loss: 2.7141 | Train NLL: 1.3570 | Val Loss: 2.8311 | Val NLL: 1.5071\n",
      "Epoch 14 | Train Loss: 2.6322 | Train NLL: 1.3410 | Val Loss: 2.7386 | Val NLL: 1.4748\n",
      "Epoch 15 | Train Loss: 2.5457 | Train NLL: 1.3108 | Val Loss: 2.6483 | Val NLL: 1.4397\n",
      "Epoch 16 | Train Loss: 2.4582 | Train NLL: 1.2762 | Val Loss: 2.5697 | Val NLL: 1.4111\n",
      "Epoch 17 | Train Loss: 2.3631 | Train NLL: 1.2300 | Val Loss: 2.4650 | Val NLL: 1.3559\n",
      "Epoch 18 | Train Loss: 2.2885 | Train NLL: 1.2027 | Val Loss: 2.4060 | Val NLL: 1.3407\n",
      "Epoch 19 | Train Loss: 2.2212 | Train NLL: 1.1763 | Val Loss: 2.3380 | Val NLL: 1.3127\n",
      "Epoch 20 | Train Loss: 2.1645 | Train NLL: 1.1572 | Val Loss: 2.3026 | Val NLL: 1.3128\n",
      "Epoch 21 | Train Loss: 2.1180 | Train NLL: 1.1442 | Val Loss: 2.2546 | Val NLL: 1.2967\n",
      "Epoch 22 | Train Loss: 2.0710 | Train NLL: 1.1276 | Val Loss: 2.2082 | Val NLL: 1.2786\n",
      "Epoch 23 | Train Loss: 2.0394 | Train NLL: 1.1228 | Val Loss: 2.1827 | Val NLL: 1.2779\n",
      "Epoch 24 | Train Loss: 1.9784 | Train NLL: 1.0877 | Val Loss: 2.1189 | Val NLL: 1.2409\n",
      "Epoch 25 | Train Loss: 1.9476 | Train NLL: 1.0799 | Val Loss: 2.0925 | Val NLL: 1.2369\n",
      "Epoch 26 | Train Loss: 1.9044 | Train NLL: 1.0591 | Val Loss: 2.0290 | Val NLL: 1.1942\n",
      "Epoch 27 | Train Loss: 1.8728 | Train NLL: 1.0488 | Val Loss: 2.0253 | Val NLL: 1.2103\n",
      "Epoch 28 | Train Loss: 1.8365 | Train NLL: 1.0317 | Val Loss: 1.9881 | Val NLL: 1.1916\n",
      "Epoch 29 | Train Loss: 1.8030 | Train NLL: 1.0164 | Val Loss: 1.9715 | Val NLL: 1.1925\n",
      "Epoch 30 | Train Loss: 1.7757 | Train NLL: 1.0055 | Val Loss: 1.9401 | Val NLL: 1.1768\n",
      "Epoch 31 | Train Loss: 1.7476 | Train NLL: 0.9920 | Val Loss: 1.9184 | Val NLL: 1.1680\n",
      "Epoch 32 | Train Loss: 1.7304 | Train NLL: 0.9870 | Val Loss: 1.8825 | Val NLL: 1.1443\n",
      "Epoch 33 | Train Loss: 1.7058 | Train NLL: 0.9746 | Val Loss: 1.8600 | Val NLL: 1.1340\n",
      "Epoch 34 | Train Loss: 1.6762 | Train NLL: 0.9578 | Val Loss: 1.8464 | Val NLL: 1.1321\n",
      "Epoch 35 | Train Loss: 1.6488 | Train NLL: 0.9417 | Val Loss: 1.8305 | Val NLL: 1.1280\n",
      "Epoch 36 | Train Loss: 1.6280 | Train NLL: 0.9326 | Val Loss: 1.8230 | Val NLL: 1.1308\n",
      "Epoch 37 | Train Loss: 1.6100 | Train NLL: 0.9243 | Val Loss: 1.7912 | Val NLL: 1.1088\n",
      "Epoch 38 | Train Loss: 1.5915 | Train NLL: 0.9143 | Val Loss: 1.7942 | Val NLL: 1.1202\n",
      "Epoch 39 | Train Loss: 1.5891 | Train NLL: 0.9190 | Val Loss: 1.7656 | Val NLL: 1.0981\n",
      "Epoch 40 | Train Loss: 1.5662 | Train NLL: 0.9034 | Val Loss: 1.7502 | Val NLL: 1.0903\n",
      "Epoch 41 | Train Loss: 1.5456 | Train NLL: 0.8900 | Val Loss: 1.7619 | Val NLL: 1.1093\n",
      "Epoch 42 | Train Loss: 1.5252 | Train NLL: 0.8772 | Val Loss: 1.7323 | Val NLL: 1.0868\n",
      "Epoch 43 | Train Loss: 1.5084 | Train NLL: 0.8675 | Val Loss: 1.7347 | Val NLL: 1.0962\n",
      "Epoch 44 | Train Loss: 1.4978 | Train NLL: 0.8627 | Val Loss: 1.7152 | Val NLL: 1.0808\n",
      "Epoch 45 | Train Loss: 1.4808 | Train NLL: 0.8517 | Val Loss: 1.7113 | Val NLL: 1.0845\n",
      "Epoch 46 | Train Loss: 1.4649 | Train NLL: 0.8417 | Val Loss: 1.6951 | Val NLL: 1.0737\n",
      "Epoch 47 | Train Loss: 1.4523 | Train NLL: 0.8344 | Val Loss: 1.6915 | Val NLL: 1.0750\n",
      "Epoch 48 | Train Loss: 1.4416 | Train NLL: 0.8277 | Val Loss: 1.6848 | Val NLL: 1.0726\n",
      "Epoch 49 | Train Loss: 1.4281 | Train NLL: 0.8181 | Val Loss: 1.6748 | Val NLL: 1.0666\n",
      "Epoch 50 | Train Loss: 1.4169 | Train NLL: 0.8121 | Val Loss: 1.6679 | Val NLL: 1.0647\n",
      "Epoch 51 | Train Loss: 1.4045 | Train NLL: 0.8042 | Val Loss: 1.6649 | Val NLL: 1.0661\n",
      "Epoch 52 | Train Loss: 1.3913 | Train NLL: 0.7939 | Val Loss: 1.6566 | Val NLL: 1.0606\n",
      "Epoch 53 | Train Loss: 1.3826 | Train NLL: 0.7895 | Val Loss: 1.6596 | Val NLL: 1.0675\n",
      "Epoch 54 | Train Loss: 1.3708 | Train NLL: 0.7810 | Val Loss: 1.6448 | Val NLL: 1.0569\n",
      "Epoch 55 | Train Loss: 1.3598 | Train NLL: 0.7742 | Val Loss: 1.6319 | Val NLL: 1.0469\n",
      "Epoch 56 | Train Loss: 1.3472 | Train NLL: 0.7649 | Val Loss: 1.6318 | Val NLL: 1.0507\n",
      "Epoch 57 | Train Loss: 1.3396 | Train NLL: 0.7608 | Val Loss: 1.6307 | Val NLL: 1.0530\n",
      "Epoch 58 | Train Loss: 1.3278 | Train NLL: 0.7522 | Val Loss: 1.6318 | Val NLL: 1.0570\n",
      "Epoch 59 | Train Loss: 1.3198 | Train NLL: 0.7470 | Val Loss: 1.6042 | Val NLL: 1.0318\n",
      "Epoch 60 | Train Loss: 1.3095 | Train NLL: 0.7395 | Val Loss: 1.6062 | Val NLL: 1.0375\n",
      "Epoch 61 | Train Loss: 1.2990 | Train NLL: 0.7319 | Val Loss: 1.6073 | Val NLL: 1.0412\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=1e-2, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='dependent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bf52892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 13.1914 | Train NLL: 4.7321 | Val Loss: 9.4142 | Val NLL: 3.8795\n",
      "Epoch  1 | Train Loss: 7.9584 | Train NLL: 3.4694 | Val Loss: 6.9430 | Val NLL: 3.1706\n",
      "Epoch  2 | Train Loss: 6.2270 | Train NLL: 2.8409 | Val Loss: 5.8648 | Val NLL: 2.7955\n",
      "Epoch  3 | Train Loss: 5.3696 | Train NLL: 2.5101 | Val Loss: 5.1959 | Val NLL: 2.5267\n",
      "Epoch  4 | Train Loss: 4.8080 | Train NLL: 2.2804 | Val Loss: 4.7278 | Val NLL: 2.3255\n",
      "Epoch  5 | Train Loss: 4.3407 | Train NLL: 2.0583 | Val Loss: 4.2801 | Val NLL: 2.1039\n",
      "Epoch  6 | Train Loss: 3.9773 | Train NLL: 1.8904 | Val Loss: 3.9966 | Val NLL: 1.9866\n",
      "Epoch  7 | Train Loss: 3.7060 | Train NLL: 1.7743 | Val Loss: 3.7156 | Val NLL: 1.8476\n",
      "Epoch  8 | Train Loss: 3.4727 | Train NLL: 1.6657 | Val Loss: 3.5978 | Val NLL: 1.8506\n",
      "Epoch  9 | Train Loss: 3.3019 | Train NLL: 1.6107 | Val Loss: 3.4037 | Val NLL: 1.7598\n",
      "Epoch 10 | Train Loss: 3.1507 | Train NLL: 1.5472 | Val Loss: 3.2174 | Val NLL: 1.6452\n",
      "Epoch 11 | Train Loss: 3.0061 | Train NLL: 1.4850 | Val Loss: 3.1263 | Val NLL: 1.6493\n",
      "Epoch 12 | Train Loss: 2.8923 | Train NLL: 1.4509 | Val Loss: 2.9844 | Val NLL: 1.5778\n",
      "Epoch 13 | Train Loss: 2.7658 | Train NLL: 1.3937 | Val Loss: 2.8527 | Val NLL: 1.5073\n",
      "Epoch 14 | Train Loss: 2.6567 | Train NLL: 1.3439 | Val Loss: 2.7715 | Val NLL: 1.4905\n",
      "Epoch 15 | Train Loss: 2.5497 | Train NLL: 1.2989 | Val Loss: 2.6368 | Val NLL: 1.4133\n",
      "Epoch 16 | Train Loss: 2.4807 | Train NLL: 1.2812 | Val Loss: 2.5397 | Val NLL: 1.3635\n",
      "Epoch 17 | Train Loss: 2.3709 | Train NLL: 1.2206 | Val Loss: 2.4812 | Val NLL: 1.3539\n",
      "Epoch 18 | Train Loss: 2.3203 | Train NLL: 1.2181 | Val Loss: 2.4571 | Val NLL: 1.3787\n",
      "Epoch 19 | Train Loss: 2.2693 | Train NLL: 1.2082 | Val Loss: 2.3480 | Val NLL: 1.3052\n",
      "Epoch 20 | Train Loss: 2.1842 | Train NLL: 1.1604 | Val Loss: 2.3267 | Val NLL: 1.3222\n",
      "Epoch 21 | Train Loss: 2.1367 | Train NLL: 1.1471 | Val Loss: 2.2613 | Val NLL: 1.2875\n",
      "Epoch 22 | Train Loss: 2.0930 | Train NLL: 1.1346 | Val Loss: 2.1991 | Val NLL: 1.2545\n",
      "Epoch 23 | Train Loss: 2.0545 | Train NLL: 1.1248 | Val Loss: 2.1706 | Val NLL: 1.2556\n",
      "Epoch 24 | Train Loss: 2.0107 | Train NLL: 1.1071 | Val Loss: 2.1294 | Val NLL: 1.2372\n",
      "Epoch 25 | Train Loss: 1.9595 | Train NLL: 1.0802 | Val Loss: 2.0746 | Val NLL: 1.2068\n",
      "Epoch 26 | Train Loss: 1.9243 | Train NLL: 1.0701 | Val Loss: 2.0888 | Val NLL: 1.2472\n",
      "Epoch 27 | Train Loss: 1.9049 | Train NLL: 1.0709 | Val Loss: 2.0097 | Val NLL: 1.1847\n",
      "Epoch 28 | Train Loss: 1.8589 | Train NLL: 1.0433 | Val Loss: 1.9853 | Val NLL: 1.1781\n",
      "Epoch 29 | Train Loss: 1.8132 | Train NLL: 1.0185 | Val Loss: 1.9632 | Val NLL: 1.1790\n",
      "Epoch 30 | Train Loss: 1.7957 | Train NLL: 1.0182 | Val Loss: 1.9307 | Val NLL: 1.1591\n",
      "Epoch 31 | Train Loss: 1.7616 | Train NLL: 0.9989 | Val Loss: 1.8929 | Val NLL: 1.1380\n",
      "Epoch 32 | Train Loss: 1.7433 | Train NLL: 0.9963 | Val Loss: 1.8901 | Val NLL: 1.1509\n",
      "Epoch 33 | Train Loss: 1.7154 | Train NLL: 0.9816 | Val Loss: 1.8516 | Val NLL: 1.1231\n",
      "Epoch 34 | Train Loss: 1.6782 | Train NLL: 0.9583 | Val Loss: 1.8163 | Val NLL: 1.1021\n",
      "Epoch 35 | Train Loss: 1.6566 | Train NLL: 0.9488 | Val Loss: 1.8167 | Val NLL: 1.1134\n",
      "Epoch 36 | Train Loss: 1.6358 | Train NLL: 0.9390 | Val Loss: 1.7811 | Val NLL: 1.0877\n",
      "Epoch 37 | Train Loss: 1.6176 | Train NLL: 0.9305 | Val Loss: 1.7843 | Val NLL: 1.1011\n",
      "Epoch 38 | Train Loss: 1.5983 | Train NLL: 0.9203 | Val Loss: 1.7559 | Val NLL: 1.0807\n",
      "Epoch 39 | Train Loss: 1.5829 | Train NLL: 0.9131 | Val Loss: 1.7440 | Val NLL: 1.0783\n",
      "Epoch 40 | Train Loss: 1.5651 | Train NLL: 0.9042 | Val Loss: 1.7226 | Val NLL: 1.0643\n",
      "Epoch 41 | Train Loss: 1.5464 | Train NLL: 0.8922 | Val Loss: 1.7004 | Val NLL: 1.0499\n",
      "Epoch 42 | Train Loss: 1.5269 | Train NLL: 0.8807 | Val Loss: 1.7040 | Val NLL: 1.0605\n",
      "Epoch 43 | Train Loss: 1.5150 | Train NLL: 0.8758 | Val Loss: 1.6811 | Val NLL: 1.0451\n",
      "Epoch 44 | Train Loss: 1.4958 | Train NLL: 0.8630 | Val Loss: 1.6841 | Val NLL: 1.0530\n",
      "Epoch 45 | Train Loss: 1.4874 | Train NLL: 0.8606 | Val Loss: 1.6637 | Val NLL: 1.0392\n",
      "Epoch 46 | Train Loss: 1.4697 | Train NLL: 0.8486 | Val Loss: 1.6719 | Val NLL: 1.0528\n",
      "Epoch 47 | Train Loss: 1.4605 | Train NLL: 0.8447 | Val Loss: 1.6527 | Val NLL: 1.0386\n",
      "Epoch 48 | Train Loss: 1.4426 | Train NLL: 0.8312 | Val Loss: 1.6355 | Val NLL: 1.0250\n",
      "Epoch 49 | Train Loss: 1.4328 | Train NLL: 0.8261 | Val Loss: 1.6225 | Val NLL: 1.0180\n",
      "Epoch 50 | Train Loss: 1.4217 | Train NLL: 0.8194 | Val Loss: 1.6095 | Val NLL: 1.0090\n",
      "Epoch 51 | Train Loss: 1.4069 | Train NLL: 0.8097 | Val Loss: 1.6106 | Val NLL: 1.0152\n",
      "Epoch 52 | Train Loss: 1.3976 | Train NLL: 0.8036 | Val Loss: 1.5940 | Val NLL: 1.0016\n",
      "Epoch 53 | Train Loss: 1.3826 | Train NLL: 0.7934 | Val Loss: 1.5900 | Val NLL: 1.0022\n",
      "Epoch 54 | Train Loss: 1.3710 | Train NLL: 0.7854 | Val Loss: 1.5911 | Val NLL: 1.0062\n",
      "Epoch 55 | Train Loss: 1.3635 | Train NLL: 0.7818 | Val Loss: 1.5772 | Val NLL: 0.9971\n",
      "Epoch 56 | Train Loss: 1.3505 | Train NLL: 0.7730 | Val Loss: 1.5763 | Val NLL: 0.9999\n",
      "Epoch 57 | Train Loss: 1.3398 | Train NLL: 0.7653 | Val Loss: 1.5707 | Val NLL: 0.9968\n",
      "Epoch 58 | Train Loss: 1.3280 | Train NLL: 0.7571 | Val Loss: 1.5586 | Val NLL: 0.9886\n",
      "Epoch 59 | Train Loss: 1.3222 | Train NLL: 0.7544 | Val Loss: 1.5567 | Val NLL: 0.9897\n",
      "Epoch 60 | Train Loss: 1.3125 | Train NLL: 0.7472 | Val Loss: 1.5585 | Val NLL: 0.9928\n",
      "Epoch 61 | Train Loss: 1.3045 | Train NLL: 0.7415 | Val Loss: 1.5562 | Val NLL: 0.9942\n",
      "Epoch 62 | Train Loss: 1.2936 | Train NLL: 0.7344 | Val Loss: 1.5393 | Val NLL: 0.9810\n",
      "Epoch 63 | Train Loss: 1.2833 | Train NLL: 0.7271 | Val Loss: 1.5391 | Val NLL: 0.9830\n",
      "Epoch 64 | Train Loss: 1.2753 | Train NLL: 0.7217 | Val Loss: 1.5447 | Val NLL: 0.9911\n",
      "Epoch 65 | Train Loss: 1.2681 | Train NLL: 0.7166 | Val Loss: 1.5286 | Val NLL: 0.9778\n",
      "Epoch 66 | Train Loss: 1.2575 | Train NLL: 0.7082 | Val Loss: 1.5276 | Val NLL: 0.9791\n",
      "Epoch 67 | Train Loss: 1.2498 | Train NLL: 0.7029 | Val Loss: 1.5323 | Val NLL: 0.9855\n",
      "Epoch 68 | Train Loss: 1.2446 | Train NLL: 0.7002 | Val Loss: 1.5248 | Val NLL: 0.9799\n",
      "Epoch 69 | Train Loss: 1.2340 | Train NLL: 0.6913 | Val Loss: 1.5120 | Val NLL: 0.9696\n",
      "Epoch 70 | Train Loss: 1.2290 | Train NLL: 0.6892 | Val Loss: 1.5127 | Val NLL: 0.9734\n",
      "Epoch 71 | Train Loss: 1.2217 | Train NLL: 0.6842 | Val Loss: 1.5170 | Val NLL: 0.9794\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=3e-3, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='dependent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a8eff5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 13.0191 | Train NLL: 4.6394 | Val Loss: 9.3383 | Val NLL: 3.8777\n",
      "Epoch  1 | Train Loss: 7.8458 | Train NLL: 3.4167 | Val Loss: 6.9831 | Val NLL: 3.2708\n",
      "Epoch  2 | Train Loss: 6.1644 | Train NLL: 2.8422 | Val Loss: 5.8412 | Val NLL: 2.8429\n",
      "Epoch  3 | Train Loss: 5.2276 | Train NLL: 2.4518 | Val Loss: 5.1276 | Val NLL: 2.5466\n",
      "Epoch  4 | Train Loss: 4.6598 | Train NLL: 2.2263 | Val Loss: 4.5407 | Val NLL: 2.2418\n",
      "Epoch  5 | Train Loss: 4.2351 | Train NLL: 2.0460 | Val Loss: 4.1471 | Val NLL: 2.0593\n",
      "Epoch  6 | Train Loss: 3.8506 | Train NLL: 1.8497 | Val Loss: 3.8410 | Val NLL: 1.9212\n",
      "Epoch  7 | Train Loss: 3.6075 | Train NLL: 1.7566 | Val Loss: 3.5697 | Val NLL: 1.7841\n",
      "Epoch  8 | Train Loss: 3.4198 | Train NLL: 1.6925 | Val Loss: 3.4113 | Val NLL: 1.7377\n",
      "Epoch  9 | Train Loss: 3.2026 | Train NLL: 1.5795 | Val Loss: 3.1868 | Val NLL: 1.6130\n",
      "Epoch 10 | Train Loss: 3.0300 | Train NLL: 1.5014 | Val Loss: 3.0317 | Val NLL: 1.5450\n",
      "Epoch 11 | Train Loss: 2.9009 | Train NLL: 1.4550 | Val Loss: 2.9623 | Val NLL: 1.5511\n",
      "Epoch 12 | Train Loss: 2.7909 | Train NLL: 1.4172 | Val Loss: 2.8569 | Val NLL: 1.5136\n",
      "Epoch 13 | Train Loss: 2.6922 | Train NLL: 1.3820 | Val Loss: 2.7460 | Val NLL: 1.4634\n",
      "Epoch 14 | Train Loss: 2.5669 | Train NLL: 1.3168 | Val Loss: 2.6476 | Val NLL: 1.4260\n",
      "Epoch 15 | Train Loss: 2.4857 | Train NLL: 1.2930 | Val Loss: 2.5552 | Val NLL: 1.3876\n",
      "Epoch 16 | Train Loss: 2.4021 | Train NLL: 1.2602 | Val Loss: 2.4938 | Val NLL: 1.3735\n",
      "Epoch 17 | Train Loss: 2.3288 | Train NLL: 1.2317 | Val Loss: 2.4173 | Val NLL: 1.3396\n",
      "Epoch 18 | Train Loss: 2.2699 | Train NLL: 1.2129 | Val Loss: 2.3734 | Val NLL: 1.3349\n",
      "Epoch 19 | Train Loss: 2.2115 | Train NLL: 1.1920 | Val Loss: 2.3245 | Val NLL: 1.3206\n",
      "Epoch 20 | Train Loss: 2.1664 | Train NLL: 1.1802 | Val Loss: 2.2798 | Val NLL: 1.3079\n",
      "Epoch 21 | Train Loss: 2.1115 | Train NLL: 1.1565 | Val Loss: 2.2546 | Val NLL: 1.3127\n",
      "Epoch 22 | Train Loss: 2.0671 | Train NLL: 1.1409 | Val Loss: 2.2009 | Val NLL: 1.2873\n",
      "Epoch 23 | Train Loss: 2.0197 | Train NLL: 1.1206 | Val Loss: 2.1605 | Val NLL: 1.2730\n",
      "Epoch 24 | Train Loss: 1.9799 | Train NLL: 1.1055 | Val Loss: 2.1223 | Val NLL: 1.2581\n",
      "Epoch 25 | Train Loss: 1.9551 | Train NLL: 1.1018 | Val Loss: 2.1007 | Val NLL: 1.2571\n",
      "Epoch 26 | Train Loss: 1.9376 | Train NLL: 1.1028 | Val Loss: 2.0463 | Val NLL: 1.2199\n",
      "Epoch 27 | Train Loss: 1.8783 | Train NLL: 1.0647 | Val Loss: 2.0230 | Val NLL: 1.2178\n",
      "Epoch 28 | Train Loss: 1.8647 | Train NLL: 1.0684 | Val Loss: 1.9888 | Val NLL: 1.2006\n",
      "Epoch 29 | Train Loss: 1.8361 | Train NLL: 1.0562 | Val Loss: 1.9461 | Val NLL: 1.1731\n",
      "Epoch 30 | Train Loss: 1.7997 | Train NLL: 1.0355 | Val Loss: 1.9148 | Val NLL: 1.1571\n",
      "Epoch 31 | Train Loss: 1.7684 | Train NLL: 1.0193 | Val Loss: 1.8977 | Val NLL: 1.1531\n",
      "Epoch 32 | Train Loss: 1.7371 | Train NLL: 1.0015 | Val Loss: 1.8721 | Val NLL: 1.1426\n",
      "Epoch 33 | Train Loss: 1.7084 | Train NLL: 0.9864 | Val Loss: 1.8499 | Val NLL: 1.1330\n",
      "Epoch 34 | Train Loss: 1.7013 | Train NLL: 0.9912 | Val Loss: 1.8556 | Val NLL: 1.1499\n",
      "Epoch 35 | Train Loss: 1.6753 | Train NLL: 0.9751 | Val Loss: 1.8079 | Val NLL: 1.1119\n",
      "Epoch 36 | Train Loss: 1.6443 | Train NLL: 0.9545 | Val Loss: 1.8080 | Val NLL: 1.1226\n",
      "Epoch 37 | Train Loss: 1.6249 | Train NLL: 0.9462 | Val Loss: 1.7841 | Val NLL: 1.1085\n",
      "Epoch 38 | Train Loss: 1.6045 | Train NLL: 0.9346 | Val Loss: 1.7804 | Val NLL: 1.1132\n",
      "Epoch 39 | Train Loss: 1.5882 | Train NLL: 0.9262 | Val Loss: 1.7762 | Val NLL: 1.1166\n",
      "Epoch 40 | Train Loss: 1.5785 | Train NLL: 0.9239 | Val Loss: 1.7632 | Val NLL: 1.1112\n",
      "Epoch 41 | Train Loss: 1.5655 | Train NLL: 0.9181 | Val Loss: 1.7403 | Val NLL: 1.0949\n",
      "Epoch 42 | Train Loss: 1.5425 | Train NLL: 0.9026 | Val Loss: 1.7596 | Val NLL: 1.1206\n",
      "Epoch 43 | Train Loss: 1.5275 | Train NLL: 0.8932 | Val Loss: 1.7123 | Val NLL: 1.0797\n",
      "Epoch 44 | Train Loss: 1.5105 | Train NLL: 0.8827 | Val Loss: 1.7289 | Val NLL: 1.1014\n",
      "Epoch 45 | Train Loss: 1.4992 | Train NLL: 0.8772 | Val Loss: 1.6941 | Val NLL: 1.0733\n",
      "Epoch 46 | Train Loss: 1.4817 | Train NLL: 0.8653 | Val Loss: 1.6962 | Val NLL: 1.0807\n",
      "Epoch 47 | Train Loss: 1.4728 | Train NLL: 0.8607 | Val Loss: 1.6788 | Val NLL: 1.0678\n",
      "Epoch 48 | Train Loss: 1.4682 | Train NLL: 0.8597 | Val Loss: 1.6792 | Val NLL: 1.0712\n",
      "Epoch 49 | Train Loss: 1.4504 | Train NLL: 0.8470 | Val Loss: 1.6626 | Val NLL: 1.0603\n",
      "Epoch 50 | Train Loss: 1.4331 | Train NLL: 0.8345 | Val Loss: 1.6705 | Val NLL: 1.0724\n",
      "Epoch 51 | Train Loss: 1.4225 | Train NLL: 0.8275 | Val Loss: 1.6555 | Val NLL: 1.0602\n",
      "Epoch 52 | Train Loss: 1.4067 | Train NLL: 0.8152 | Val Loss: 1.6513 | Val NLL: 1.0610\n",
      "Epoch 53 | Train Loss: 1.3942 | Train NLL: 0.8071 | Val Loss: 1.6343 | Val NLL: 1.0477\n",
      "Epoch 54 | Train Loss: 1.3859 | Train NLL: 0.8027 | Val Loss: 1.6517 | Val NLL: 1.0682\n",
      "Epoch 55 | Train Loss: 1.3778 | Train NLL: 0.7977 | Val Loss: 1.6307 | Val NLL: 1.0515\n",
      "Epoch 56 | Train Loss: 1.3604 | Train NLL: 0.7833 | Val Loss: 1.6151 | Val NLL: 1.0389\n",
      "Epoch 57 | Train Loss: 1.3539 | Train NLL: 0.7808 | Val Loss: 1.6092 | Val NLL: 1.0380\n",
      "Epoch 58 | Train Loss: 1.3445 | Train NLL: 0.7747 | Val Loss: 1.6113 | Val NLL: 1.0427\n",
      "Epoch 59 | Train Loss: 1.3350 | Train NLL: 0.7684 | Val Loss: 1.6147 | Val NLL: 1.0488\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=3e-2, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='dependent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7069b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 11.4524 | Train NLL: 4.6612 | Val Loss: 8.0586 | Val NLL: 3.7945\n",
      "Epoch  1 | Train Loss: 6.6149 | Train NLL: 3.2779 | Val Loss: 5.7044 | Val NLL: 2.9744\n",
      "Epoch  2 | Train Loss: 5.1153 | Train NLL: 2.6652 | Val Loss: 4.7884 | Val NLL: 2.5526\n",
      "Epoch  3 | Train Loss: 4.4016 | Train NLL: 2.3036 | Val Loss: 4.2561 | Val NLL: 2.2758\n",
      "Epoch  4 | Train Loss: 3.9472 | Train NLL: 2.0578 | Val Loss: 3.9252 | Val NLL: 2.1161\n",
      "Epoch  5 | Train Loss: 3.6219 | Train NLL: 1.8824 | Val Loss: 3.6255 | Val NLL: 1.9454\n",
      "Epoch  6 | Train Loss: 3.3758 | Train NLL: 1.7534 | Val Loss: 3.4559 | Val NLL: 1.8815\n",
      "Epoch  7 | Train Loss: 3.1802 | Train NLL: 1.6541 | Val Loss: 3.2467 | Val NLL: 1.7615\n",
      "Epoch  8 | Train Loss: 3.0147 | Train NLL: 1.5701 | Val Loss: 3.0640 | Val NLL: 1.6536\n",
      "Epoch  9 | Train Loss: 2.8619 | Train NLL: 1.4878 | Val Loss: 2.9391 | Val NLL: 1.5964\n",
      "Epoch 10 | Train Loss: 2.7584 | Train NLL: 1.4484 | Val Loss: 2.8881 | Val NLL: 1.6034\n",
      "Epoch 11 | Train Loss: 2.6491 | Train NLL: 1.3940 | Val Loss: 2.7632 | Val NLL: 1.5350\n",
      "Epoch 12 | Train Loss: 2.5525 | Train NLL: 1.3514 | Val Loss: 2.6900 | Val NLL: 1.5122\n",
      "Epoch 13 | Train Loss: 2.4752 | Train NLL: 1.3217 | Val Loss: 2.6026 | Val NLL: 1.4707\n",
      "Epoch 14 | Train Loss: 2.4025 | Train NLL: 1.2920 | Val Loss: 2.5752 | Val NLL: 1.4818\n",
      "Epoch 15 | Train Loss: 2.3355 | Train NLL: 1.2623 | Val Loss: 2.5066 | Val NLL: 1.4522\n",
      "Epoch 16 | Train Loss: 2.2739 | Train NLL: 1.2371 | Val Loss: 2.4205 | Val NLL: 1.3986\n",
      "Epoch 17 | Train Loss: 2.2112 | Train NLL: 1.2064 | Val Loss: 2.3708 | Val NLL: 1.3824\n",
      "Epoch 18 | Train Loss: 2.1462 | Train NLL: 1.1736 | Val Loss: 2.2898 | Val NLL: 1.3327\n",
      "Epoch 19 | Train Loss: 2.0938 | Train NLL: 1.1513 | Val Loss: 2.2585 | Val NLL: 1.3296\n",
      "Epoch 20 | Train Loss: 2.0559 | Train NLL: 1.1402 | Val Loss: 2.2306 | Val NLL: 1.3267\n",
      "Epoch 21 | Train Loss: 2.0082 | Train NLL: 1.1164 | Val Loss: 2.1891 | Val NLL: 1.3086\n",
      "Epoch 22 | Train Loss: 1.9634 | Train NLL: 1.0941 | Val Loss: 2.1354 | Val NLL: 1.2776\n",
      "Epoch 23 | Train Loss: 1.9214 | Train NLL: 1.0741 | Val Loss: 2.1186 | Val NLL: 1.2820\n",
      "Epoch 24 | Train Loss: 1.8914 | Train NLL: 1.0645 | Val Loss: 2.0593 | Val NLL: 1.2419\n",
      "Epoch 25 | Train Loss: 1.8562 | Train NLL: 1.0473 | Val Loss: 2.0532 | Val NLL: 1.2537\n",
      "Epoch 26 | Train Loss: 1.8399 | Train NLL: 1.0480 | Val Loss: 2.0375 | Val NLL: 1.2520\n",
      "Epoch 27 | Train Loss: 1.7998 | Train NLL: 1.0226 | Val Loss: 1.9840 | Val NLL: 1.2157\n",
      "Epoch 28 | Train Loss: 1.7689 | Train NLL: 1.0080 | Val Loss: 1.9666 | Val NLL: 1.2130\n",
      "Epoch 29 | Train Loss: 1.7402 | Train NLL: 0.9936 | Val Loss: 1.9446 | Val NLL: 1.2054\n",
      "Epoch 30 | Train Loss: 1.7254 | Train NLL: 0.9916 | Val Loss: 1.9293 | Val NLL: 1.2012\n",
      "Epoch 31 | Train Loss: 1.6931 | Train NLL: 0.9712 | Val Loss: 1.9095 | Val NLL: 1.1938\n",
      "Epoch 32 | Train Loss: 1.6710 | Train NLL: 0.9617 | Val Loss: 1.8813 | Val NLL: 1.1780\n",
      "Epoch 33 | Train Loss: 1.6508 | Train NLL: 0.9522 | Val Loss: 1.8699 | Val NLL: 1.1771\n",
      "Epoch 34 | Train Loss: 1.6311 | Train NLL: 0.9429 | Val Loss: 1.8264 | Val NLL: 1.1438\n",
      "Epoch 35 | Train Loss: 1.6097 | Train NLL: 0.9316 | Val Loss: 1.8244 | Val NLL: 1.1508\n",
      "Epoch 36 | Train Loss: 1.5884 | Train NLL: 0.9196 | Val Loss: 1.8062 | Val NLL: 1.1424\n",
      "Epoch 37 | Train Loss: 1.5711 | Train NLL: 0.9111 | Val Loss: 1.7911 | Val NLL: 1.1353\n",
      "Epoch 38 | Train Loss: 1.5575 | Train NLL: 0.9048 | Val Loss: 1.7589 | Val NLL: 1.1105\n",
      "Epoch 39 | Train Loss: 1.5364 | Train NLL: 0.8917 | Val Loss: 1.7670 | Val NLL: 1.1268\n",
      "Epoch 40 | Train Loss: 1.5216 | Train NLL: 0.8844 | Val Loss: 1.7434 | Val NLL: 1.1098\n",
      "Epoch 41 | Train Loss: 1.5097 | Train NLL: 0.8781 | Val Loss: 1.7274 | Val NLL: 1.0992\n",
      "Epoch 42 | Train Loss: 1.4901 | Train NLL: 0.8656 | Val Loss: 1.7181 | Val NLL: 1.0970\n",
      "Epoch 43 | Train Loss: 1.4740 | Train NLL: 0.8559 | Val Loss: 1.7145 | Val NLL: 1.1001\n",
      "Epoch 44 | Train Loss: 1.4606 | Train NLL: 0.8483 | Val Loss: 1.6908 | Val NLL: 1.0817\n",
      "Epoch 45 | Train Loss: 1.4725 | Train NLL: 0.8647 | Val Loss: 1.7104 | Val NLL: 1.1042\n",
      "Epoch 46 | Train Loss: 1.4607 | Train NLL: 0.8547 | Val Loss: 1.6892 | Val NLL: 1.0853\n",
      "Epoch 47 | Train Loss: 1.4313 | Train NLL: 0.8305 | Val Loss: 1.6915 | Val NLL: 1.0945\n",
      "Epoch 48 | Train Loss: 1.4107 | Train NLL: 0.8152 | Val Loss: 1.6797 | Val NLL: 1.0876\n",
      "Epoch 49 | Train Loss: 1.3964 | Train NLL: 0.8056 | Val Loss: 1.6837 | Val NLL: 1.0951\n",
      "Epoch 50 | Train Loss: 1.3857 | Train NLL: 0.7981 | Val Loss: 1.6629 | Val NLL: 1.0782\n",
      "Epoch 51 | Train Loss: 1.3728 | Train NLL: 0.7898 | Val Loss: 1.6728 | Val NLL: 1.0929\n",
      "Epoch 52 | Train Loss: 1.3573 | Train NLL: 0.7786 | Val Loss: 1.6609 | Val NLL: 1.0853\n",
      "Epoch 53 | Train Loss: 1.3464 | Train NLL: 0.7714 | Val Loss: 1.6489 | Val NLL: 1.0772\n",
      "Epoch 54 | Train Loss: 1.3372 | Train NLL: 0.7652 | Val Loss: 1.6406 | Val NLL: 1.0721\n",
      "Epoch 55 | Train Loss: 1.3286 | Train NLL: 0.7610 | Val Loss: 1.6466 | Val NLL: 1.0820\n",
      "Epoch 56 | Train Loss: 1.3190 | Train NLL: 0.7549 | Val Loss: 1.6422 | Val NLL: 1.0800\n"
     ]
    }
   ],
   "source": [
    "model = NMC(n_bins=N_BINS, importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='dependent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4db3b8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 9.6619 | Train NLL: 4.6527 | Val Loss: 7.3194 | Val NLL: 3.6637\n",
      "Epoch  1 | Train Loss: 6.2966 | Train NLL: 3.1978 | Val Loss: 5.5379 | Val NLL: 2.8351\n",
      "Epoch  2 | Train Loss: 5.0827 | Train NLL: 2.6026 | Val Loss: 4.7767 | Val NLL: 2.4807\n",
      "Epoch  3 | Train Loss: 4.4156 | Train NLL: 2.2523 | Val Loss: 4.3025 | Val NLL: 2.2553\n",
      "Epoch  4 | Train Loss: 3.9683 | Train NLL: 2.0159 | Val Loss: 4.0239 | Val NLL: 2.1558\n",
      "Epoch  5 | Train Loss: 3.6423 | Train NLL: 1.8480 | Val Loss: 3.6705 | Val NLL: 1.9430\n",
      "Epoch  6 | Train Loss: 3.3758 | Train NLL: 1.7080 | Val Loss: 3.3487 | Val NLL: 1.7328\n",
      "Epoch  7 | Train Loss: 3.1421 | Train NLL: 1.5792 | Val Loss: 3.1314 | Val NLL: 1.6156\n",
      "Epoch  8 | Train Loss: 2.9698 | Train NLL: 1.5000 | Val Loss: 2.9956 | Val NLL: 1.5650\n",
      "Epoch  9 | Train Loss: 2.8348 | Train NLL: 1.4442 | Val Loss: 2.9044 | Val NLL: 1.5479\n",
      "Epoch 10 | Train Loss: 2.7052 | Train NLL: 1.3855 | Val Loss: 2.7613 | Val NLL: 1.4721\n",
      "Epoch 11 | Train Loss: 2.6284 | Train NLL: 1.3717 | Val Loss: 2.7223 | Val NLL: 1.4892\n",
      "Epoch 12 | Train Loss: 2.5211 | Train NLL: 1.3179 | Val Loss: 2.5620 | Val NLL: 1.3865\n",
      "Epoch 13 | Train Loss: 2.4330 | Train NLL: 1.2837 | Val Loss: 2.4837 | Val NLL: 1.3558\n",
      "Epoch 14 | Train Loss: 2.3527 | Train NLL: 1.2495 | Val Loss: 2.4204 | Val NLL: 1.3378\n",
      "Epoch 15 | Train Loss: 2.2816 | Train NLL: 1.2213 | Val Loss: 2.3554 | Val NLL: 1.3139\n",
      "Epoch 16 | Train Loss: 2.2188 | Train NLL: 1.1977 | Val Loss: 2.2981 | Val NLL: 1.2942\n",
      "Epoch 17 | Train Loss: 2.1765 | Train NLL: 1.1902 | Val Loss: 2.2869 | Val NLL: 1.3139\n",
      "Epoch 18 | Train Loss: 2.1248 | Train NLL: 1.1683 | Val Loss: 2.2060 | Val NLL: 1.2644\n",
      "Epoch 19 | Train Loss: 2.0835 | Train NLL: 1.1571 | Val Loss: 2.2164 | Val NLL: 1.3013\n",
      "Epoch 20 | Train Loss: 2.0380 | Train NLL: 1.1368 | Val Loss: 2.1693 | Val NLL: 1.2800\n",
      "Epoch 21 | Train Loss: 2.0005 | Train NLL: 1.1241 | Val Loss: 2.1658 | Val NLL: 1.2989\n",
      "Epoch 22 | Train Loss: 1.9587 | Train NLL: 1.1031 | Val Loss: 2.1440 | Val NLL: 1.2984\n",
      "Epoch 23 | Train Loss: 1.9156 | Train NLL: 1.0811 | Val Loss: 2.0954 | Val NLL: 1.2704\n",
      "Epoch 24 | Train Loss: 1.8811 | Train NLL: 1.0668 | Val Loss: 2.0678 | Val NLL: 1.2627\n",
      "Epoch 25 | Train Loss: 1.8454 | Train NLL: 1.0492 | Val Loss: 2.0230 | Val NLL: 1.2351\n",
      "Epoch 26 | Train Loss: 1.8097 | Train NLL: 1.0306 | Val Loss: 1.9767 | Val NLL: 1.2057\n",
      "Epoch 27 | Train Loss: 1.7844 | Train NLL: 1.0214 | Val Loss: 1.9623 | Val NLL: 1.2064\n",
      "Epoch 28 | Train Loss: 1.7525 | Train NLL: 1.0046 | Val Loss: 1.9158 | Val NLL: 1.1747\n",
      "Epoch 29 | Train Loss: 1.7248 | Train NLL: 0.9909 | Val Loss: 1.8967 | Val NLL: 1.1681\n",
      "Epoch 30 | Train Loss: 1.7024 | Train NLL: 0.9805 | Val Loss: 1.8738 | Val NLL: 1.1579\n",
      "Epoch 31 | Train Loss: 1.6797 | Train NLL: 0.9693 | Val Loss: 1.8437 | Val NLL: 1.1385\n",
      "Epoch 32 | Train Loss: 1.6549 | Train NLL: 0.9560 | Val Loss: 1.8453 | Val NLL: 1.1507\n",
      "Epoch 33 | Train Loss: 1.6350 | Train NLL: 0.9462 | Val Loss: 1.8036 | Val NLL: 1.1197\n",
      "Epoch 34 | Train Loss: 1.6138 | Train NLL: 0.9356 | Val Loss: 1.7887 | Val NLL: 1.1148\n",
      "Epoch 35 | Train Loss: 1.5946 | Train NLL: 0.9253 | Val Loss: 1.7794 | Val NLL: 1.1135\n",
      "Epoch 36 | Train Loss: 1.5752 | Train NLL: 0.9142 | Val Loss: 1.7586 | Val NLL: 1.1015\n",
      "Epoch 37 | Train Loss: 1.5609 | Train NLL: 0.9078 | Val Loss: 1.7498 | Val NLL: 1.0997\n",
      "Epoch 38 | Train Loss: 1.5418 | Train NLL: 0.8960 | Val Loss: 1.7433 | Val NLL: 1.1003\n",
      "Epoch 39 | Train Loss: 1.5283 | Train NLL: 0.8892 | Val Loss: 1.7165 | Val NLL: 1.0808\n",
      "Epoch 40 | Train Loss: 1.5102 | Train NLL: 0.8777 | Val Loss: 1.7133 | Val NLL: 1.0825\n",
      "Epoch 41 | Train Loss: 1.4949 | Train NLL: 0.8685 | Val Loss: 1.7003 | Val NLL: 1.0772\n",
      "Epoch 42 | Train Loss: 1.4831 | Train NLL: 0.8633 | Val Loss: 1.6892 | Val NLL: 1.0715\n",
      "Epoch 43 | Train Loss: 1.4715 | Train NLL: 0.8569 | Val Loss: 1.6745 | Val NLL: 1.0621\n",
      "Epoch 44 | Train Loss: 1.4538 | Train NLL: 0.8444 | Val Loss: 1.6870 | Val NLL: 1.0794\n",
      "Epoch 45 | Train Loss: 1.4509 | Train NLL: 0.8460 | Val Loss: 1.6627 | Val NLL: 1.0600\n",
      "Epoch 46 | Train Loss: 1.4341 | Train NLL: 0.8345 | Val Loss: 1.6500 | Val NLL: 1.0524\n",
      "Epoch 47 | Train Loss: 1.4172 | Train NLL: 0.8216 | Val Loss: 1.6437 | Val NLL: 1.0492\n",
      "Epoch 48 | Train Loss: 1.4063 | Train NLL: 0.8151 | Val Loss: 1.6380 | Val NLL: 1.0491\n",
      "Epoch 49 | Train Loss: 1.4009 | Train NLL: 0.8136 | Val Loss: 1.6384 | Val NLL: 1.0531\n",
      "Epoch 50 | Train Loss: 1.3868 | Train NLL: 0.8040 | Val Loss: 1.6201 | Val NLL: 1.0393\n",
      "Epoch 51 | Train Loss: 1.3749 | Train NLL: 0.7960 | Val Loss: 1.6172 | Val NLL: 1.0398\n",
      "Epoch 52 | Train Loss: 1.3679 | Train NLL: 0.7924 | Val Loss: 1.6175 | Val NLL: 1.0437\n",
      "Epoch 53 | Train Loss: 1.3556 | Train NLL: 0.7839 | Val Loss: 1.6050 | Val NLL: 1.0349\n",
      "Epoch 54 | Train Loss: 1.3484 | Train NLL: 0.7795 | Val Loss: 1.6038 | Val NLL: 1.0360\n",
      "Epoch 55 | Train Loss: 1.3336 | Train NLL: 0.7678 | Val Loss: 1.5900 | Val NLL: 1.0256\n",
      "Epoch 56 | Train Loss: 1.3234 | Train NLL: 0.7609 | Val Loss: 1.5933 | Val NLL: 1.0321\n",
      "Epoch 57 | Train Loss: 1.3125 | Train NLL: 0.7532 | Val Loss: 1.5873 | Val NLL: 1.0294\n",
      "Epoch 58 | Train Loss: 1.3058 | Train NLL: 0.7496 | Val Loss: 1.5798 | Val NLL: 1.0254\n",
      "Epoch 59 | Train Loss: 1.2968 | Train NLL: 0.7432 | Val Loss: 1.5819 | Val NLL: 1.0301\n",
      "Epoch 60 | Train Loss: 1.2900 | Train NLL: 0.7389 | Val Loss: 1.5765 | Val NLL: 1.0267\n",
      "Epoch 61 | Train Loss: 1.2747 | Train NLL: 0.7269 | Val Loss: 1.5668 | Val NLL: 1.0200\n",
      "Epoch 62 | Train Loss: 1.2663 | Train NLL: 0.7209 | Val Loss: 1.5659 | Val NLL: 1.0221\n",
      "Epoch 63 | Train Loss: 1.2677 | Train NLL: 0.7250 | Val Loss: 1.5596 | Val NLL: 1.0177\n",
      "Epoch 64 | Train Loss: 1.2498 | Train NLL: 0.7097 | Val Loss: 1.5515 | Val NLL: 1.0125\n",
      "Epoch 65 | Train Loss: 1.2465 | Train NLL: 0.7086 | Val Loss: 1.5497 | Val NLL: 1.0137\n",
      "Epoch 66 | Train Loss: 1.2351 | Train NLL: 0.7003 | Val Loss: 1.5480 | Val NLL: 1.0141\n",
      "Epoch 67 | Train Loss: 1.2310 | Train NLL: 0.6981 | Val Loss: 1.5429 | Val NLL: 1.0110\n",
      "Epoch 68 | Train Loss: 1.2188 | Train NLL: 0.6882 | Val Loss: 1.5451 | Val NLL: 1.0160\n",
      "Epoch 69 | Train Loss: 1.2174 | Train NLL: 0.6886 | Val Loss: 1.5468 | Val NLL: 1.0189\n"
     ]
    }
   ],
   "source": [
    "model = NSurv(n_bins=N_BINS, importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='dependent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6310b673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 3.6624 | Train NLL: 1.2143 | Val Loss: 2.7460 | Val NLL: 0.9991\n",
      "Epoch  1 | Train Loss: 2.3054 | Train NLL: 0.8541 | Val Loss: 2.1627 | Val NLL: 0.9298\n",
      "Epoch  2 | Train Loss: 1.9031 | Train NLL: 0.7994 | Val Loss: 1.8536 | Val NLL: 0.8588\n",
      "Epoch  3 | Train Loss: 1.6766 | Train NLL: 0.7613 | Val Loss: 1.6466 | Val NLL: 0.8027\n",
      "Epoch  4 | Train Loss: 1.5205 | Train NLL: 0.7325 | Val Loss: 1.5165 | Val NLL: 0.7792\n",
      "Epoch  5 | Train Loss: 1.4037 | Train NLL: 0.7077 | Val Loss: 1.4077 | Val NLL: 0.7500\n",
      "Epoch  6 | Train Loss: 1.3114 | Train NLL: 0.6856 | Val Loss: 1.3199 | Val NLL: 0.7234\n",
      "Epoch  7 | Train Loss: 1.2370 | Train NLL: 0.6663 | Val Loss: 1.2453 | Val NLL: 0.6983\n",
      "Epoch  8 | Train Loss: 1.1740 | Train NLL: 0.6480 | Val Loss: 1.1946 | Val NLL: 0.6884\n",
      "Epoch  9 | Train Loss: 1.1203 | Train NLL: 0.6323 | Val Loss: 1.1557 | Val NLL: 0.6847\n",
      "Epoch 10 | Train Loss: 1.0808 | Train NLL: 0.6255 | Val Loss: 1.1310 | Val NLL: 0.6902\n",
      "Epoch 11 | Train Loss: 1.0421 | Train NLL: 0.6152 | Val Loss: 1.1149 | Val NLL: 0.7006\n",
      "Epoch 12 | Train Loss: 1.0081 | Train NLL: 0.6059 | Val Loss: 1.0748 | Val NLL: 0.6838\n",
      "Epoch 13 | Train Loss: 0.9768 | Train NLL: 0.5966 | Val Loss: 1.0458 | Val NLL: 0.6750\n",
      "Epoch 14 | Train Loss: 0.9559 | Train NLL: 0.5953 | Val Loss: 1.0661 | Val NLL: 0.7142\n",
      "Epoch 15 | Train Loss: 0.9378 | Train NLL: 0.5946 | Val Loss: 1.0700 | Val NLL: 0.7346\n"
     ]
    }
   ],
   "source": [
    "model = MLP(importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='dependent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd1bec",
   "metadata": {},
   "source": [
    "# Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6f5e3a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>factors</th>\n",
       "      <th>model</th>\n",
       "      <th>ld</th>\n",
       "      <th>lr</th>\n",
       "      <th>avg_test_loss</th>\n",
       "      <th>avg_test_nll</th>\n",
       "      <th>e_auc</th>\n",
       "      <th>y_ci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>support</td>\n",
       "      <td>separate</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.465430</td>\n",
       "      <td>0.919718</td>\n",
       "      <td>0.918621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>support</td>\n",
       "      <td>separate</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.473435</td>\n",
       "      <td>0.854149</td>\n",
       "      <td>0.925540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>support</td>\n",
       "      <td>separate</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.474421</td>\n",
       "      <td>0.744926</td>\n",
       "      <td>0.921541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>support</td>\n",
       "      <td>separate</td>\n",
       "      <td>NMC</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.910901</td>\n",
       "      <td>3.650213</td>\n",
       "      <td>0.943959</td>\n",
       "      <td>0.705361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>support</td>\n",
       "      <td>separate</td>\n",
       "      <td>NSurv</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.989088</td>\n",
       "      <td>3.692924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.837757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>support</td>\n",
       "      <td>separate</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.040912</td>\n",
       "      <td>0.982602</td>\n",
       "      <td>0.923440</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>support</td>\n",
       "      <td>overlapping</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.639990</td>\n",
       "      <td>3.253772</td>\n",
       "      <td>0.685192</td>\n",
       "      <td>0.892703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>support</td>\n",
       "      <td>overlapping</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.602617</td>\n",
       "      <td>3.216988</td>\n",
       "      <td>0.567626</td>\n",
       "      <td>0.893366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>support</td>\n",
       "      <td>overlapping</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.697136</td>\n",
       "      <td>3.257908</td>\n",
       "      <td>0.639826</td>\n",
       "      <td>0.891946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>support</td>\n",
       "      <td>overlapping</td>\n",
       "      <td>NMC</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.580758</td>\n",
       "      <td>3.204116</td>\n",
       "      <td>0.470909</td>\n",
       "      <td>0.894326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>support</td>\n",
       "      <td>overlapping</td>\n",
       "      <td>NSurv</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.573346</td>\n",
       "      <td>3.194595</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.895058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>support</td>\n",
       "      <td>overlapping</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.056502</td>\n",
       "      <td>0.982877</td>\n",
       "      <td>0.927787</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>support</td>\n",
       "      <td>dependent</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.626486</td>\n",
       "      <td>1.060029</td>\n",
       "      <td>0.552526</td>\n",
       "      <td>0.923515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>support</td>\n",
       "      <td>dependent</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.576547</td>\n",
       "      <td>1.038808</td>\n",
       "      <td>0.847914</td>\n",
       "      <td>0.917947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>support</td>\n",
       "      <td>dependent</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.666736</td>\n",
       "      <td>1.099827</td>\n",
       "      <td>0.521161</td>\n",
       "      <td>0.920447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>support</td>\n",
       "      <td>dependent</td>\n",
       "      <td>NMC</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.604089</td>\n",
       "      <td>1.041883</td>\n",
       "      <td>0.548959</td>\n",
       "      <td>0.922450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>support</td>\n",
       "      <td>dependent</td>\n",
       "      <td>NSurv</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.557598</td>\n",
       "      <td>1.029681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.924128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>support</td>\n",
       "      <td>dependent</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.006188</td>\n",
       "      <td>0.670746</td>\n",
       "      <td>0.714525</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset      factors  model     ld    lr  avg_test_loss  avg_test_nll  \\\n",
       "0   support     separate   DNMC  0.010  0.01            NaN      3.465430   \n",
       "1   support     separate   DNMC  0.003  0.01            NaN      3.473435   \n",
       "2   support     separate   DNMC  0.030  0.01            NaN      3.474421   \n",
       "3   support     separate    NMC  0.001  0.01       3.910901      3.650213   \n",
       "4   support     separate  NSurv  0.001  0.01       3.989088      3.692924   \n",
       "5   support     separate    MLP  0.001  0.01       1.040912      0.982602   \n",
       "6   support  overlapping   DNMC  0.010  0.01       3.639990      3.253772   \n",
       "7   support  overlapping   DNMC  0.003  0.01       3.602617      3.216988   \n",
       "8   support  overlapping   DNMC  0.030  0.01       3.697136      3.257908   \n",
       "9   support  overlapping    NMC  0.001  0.01       3.580758      3.204116   \n",
       "10  support  overlapping  NSurv  0.001  0.01       3.573346      3.194595   \n",
       "11  support  overlapping    MLP  0.001  0.01       1.056502      0.982877   \n",
       "12  support    dependent   DNMC  0.010  0.01       1.626486      1.060029   \n",
       "13  support    dependent   DNMC  0.003  0.01       1.576547      1.038808   \n",
       "14  support    dependent   DNMC  0.030  0.01       1.666736      1.099827   \n",
       "15  support    dependent    NMC  0.001  0.01       1.604089      1.041883   \n",
       "16  support    dependent  NSurv  0.001  0.01       1.557598      1.029681   \n",
       "17  support    dependent    MLP  0.001  0.01       1.006188      0.670746   \n",
       "\n",
       "       e_auc      y_ci  \n",
       "0   0.919718  0.918621  \n",
       "1   0.854149  0.925540  \n",
       "2   0.744926  0.921541  \n",
       "3   0.943959  0.705361  \n",
       "4        NaN  0.837757  \n",
       "5   0.923440       NaN  \n",
       "6   0.685192  0.892703  \n",
       "7   0.567626  0.893366  \n",
       "8   0.639826  0.891946  \n",
       "9   0.470909  0.894326  \n",
       "10       NaN  0.895058  \n",
       "11  0.927787       NaN  \n",
       "12  0.552526  0.923515  \n",
       "13  0.847914  0.917947  \n",
       "14  0.521161  0.920447  \n",
       "15  0.548959  0.922450  \n",
       "16       NaN  0.924128  \n",
       "17  0.714525       NaN  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame(all_results)\n",
    "summary.to_csv('~/Downloads/support_results.csv', index=False)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate = non-overlapping subsets of predictors that predict occurrence vs time\n",
    "# another way to think of it is that there are just two factors in the model, vs three"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc197d",
   "metadata": {},
   "source": [
    "### Identification of Factors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64896640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trying to work toward evidence of identification of factors\n",
    "# # Phi -> E, Omega -> T\n",
    "\n",
    "# E_mask = np.array([False] * X.shape[1])\n",
    "# E_mask[E_features] = True\n",
    "\n",
    "# T_mask = np.array([False] * X.shape[1])\n",
    "# T_mask[T_features] = True\n",
    "\n",
    "# phi_on_e = np.abs(model.phi_layers[0].kernel.numpy()).max(axis=1)[E_mask].mean()\n",
    "# phi_off_e = np.abs(model.phi_layers[0].kernel.numpy()).max(axis=1)[~E_mask].mean()\n",
    "\n",
    "# omega_on_t = np.abs(model.omega_layers[0].kernel.numpy()).max(axis=1)[T_mask].mean()\n",
    "# omega_off_t = np.abs(model.omega_layers[0].kernel.numpy()).max(axis=1)[~T_mask].mean()\n",
    "\n",
    "# print(phi_on_e, phi_off_e, omega_on_t, omega_off_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
